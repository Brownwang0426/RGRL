{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cloning git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/Brownwang0426/RGRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "3350c060-7302-4eeb-bfc9-f73f03974e0e"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0y9RfWif-v2",
        "outputId": "31b7bb14-f74e-4aaf-fe69-4cebbdb8bbcf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "af231b3d-b797-492f-b1ad-2d5ce1b94ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA T500\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'     # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "max_steps_for_each_episode = 200  # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "ensemble_size = 5                 # (Reminder: change this to see MWM-SGD's magic ◀️◀️◀️) choose the size of the neural ensemble \n",
        "state_size =  200                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 3                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 50                    # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'gru'               # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your neural type: [rnn, gru, lstm] [att]\n",
        "num_layers = 2                    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose the number of layers for your rnn or attention: [1, 2, 3, 4, etc.]\n",
        "num_heads = None                  # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "shift = 0.0                       # choose shift for output \n",
        "init = \"random_normal\"            # choose initialization method: [random_normal, random_uniform, xavier_normal, xavier_uniform, glorot_normal, glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam, sgd, rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error, binary_crossentropy]\n",
        "drop_rate = 0.001                 # (Reminder: change this to see dropout's magic ◀️◀️◀️) choose your drop rate \n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "iteration_for_learning = 20000    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose learning iteration for nn weights \n",
        "mask_value = sys.maxsize          # mask value\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1                       # gaussian noise\n",
        "noise_r = 0.1                     # (Reminder: change this for your specific task ⚠️⚠️⚠️) smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time \n",
        "beta = 0.1                        # updating rate for input actions\n",
        "iteration_for_deducing = 200      # (Reminder: change this for your specific task ⚠️⚠️⚠️) updating iteration for input actions\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 1 # batch size for batch offline learning\n",
        "PER_epsilon = 0.000001              # prioritized_experience_replay epsilon\n",
        "PER_exponent = 1                    # prioritized_experience_replay exponent\n",
        "EWC_lambda = 1                      # elastic weight control lambda \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"      # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "max_steps_for_each_episode = 200  # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "ensemble_size = 10                # (Reminder: change this to see MWM-SGD's magic ◀️◀️◀️) choose the size of the neural ensemble \n",
        "state_size =  800                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 250                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 4                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 50                    # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 250                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'gru'               # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your neural type: [rnn, gru, lstm] [att]\n",
        "num_layers = 2                    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose the number of layers for your rnn or attention: [1, 2, 3, 4, etc.]\n",
        "num_heads = None                  # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "shift = 0.0                       # choose shift for output \n",
        "init = \"random_normal\"            # choose initialization method: [random_normal, random_uniform, xavier_normal, xavier_uniform, glorot_normal, glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam, sgd, rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error, binary_crossentropy]\n",
        "drop_rate = 0.001                 # (Reminder: change this to see dropout's magic ◀️◀️◀️) choose your drop rate \n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "iteration_for_learning = 20000    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose learning iteration for nn weights \n",
        "mask_value = sys.maxsize          # mask value\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1                       # gaussian noise\n",
        "noise_r = 0.1                     # (Reminder: change this for your specific task ⚠️⚠️⚠️) smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time \n",
        "beta = 0.1                        # updating rate for input actions\n",
        "iteration_for_deducing = 200      # (Reminder: change this for your specific task ⚠️⚠️⚠️) updating iteration for input actions\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 1 # batch size for batch offline learning\n",
        "PER_epsilon = 0.000001              # prioritized_experience_replay epsilon\n",
        "PER_exponent = 1                    # prioritized_experience_replay exponent\n",
        "EWC_lambda = 1                      # elastic weight control lambda \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'         # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "max_steps_for_each_episode = 2000 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "ensemble_size = 10                # (Reminder: change this to see MWM-SGD's magic ◀️◀️◀️) choose the size of the neural ensemble \n",
        "state_size =  500                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 2                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 15                    # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'gru'               # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your neural type: [rnn, gru, lstm] [att]\n",
        "num_layers = 2                    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose the number of layers for your rnn or attention: [1, 2, 3, 4, etc.]\n",
        "num_heads = None                  # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu, leaky_relu, sigmoid, tanh]\n",
        "shift = 0.0                       # choose shift for output \n",
        "init = \"random_normal\"            # choose initialization method: [random_normal, random_uniform, xavier_normal, xavier_uniform, glorot_normal, glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam, sgd, rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error, binary_crossentropy]\n",
        "drop_rate = 0.001                 # (Reminder: change this to see dropout's magic ◀️◀️◀️) choose your drop rate \n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "iteration_for_learning = 20000    # (Reminder: change this for your specific task ⚠️⚠️⚠️) choose learning iteration for nn weights \n",
        "mask_value = sys.maxsize          # mask value\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1                       # gaussian noise\n",
        "noise_r = 0.1                     # (Reminder: change this for your specific task ⚠️⚠️⚠️) smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time \n",
        "beta = 0.1                        # updating rate for input actions\n",
        "iteration_for_deducing = 200      # (Reminder: change this for your specific task ⚠️⚠️⚠️) updating iteration for input actions\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 1 # batch size for batch offline learning\n",
        "PER_epsilon = 0.000001              # prioritized_experience_replay epsilon\n",
        "PER_exponent = 1                    # prioritized_experience_replay exponent\n",
        "EWC_lambda = 1                      # elastic weight control lambda \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if   game_name == 'CartPole-v1':\n",
        "    from envs.env_cartpole    import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCar-v0':\n",
        "    from envs.env_mountaincar import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"LunarLander-v2\":\n",
        "    from envs.env_lunarlander import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "else:\n",
        "   raise RuntimeError('missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if neural_type == 'att':\n",
        "    from models.model_att import build_model\n",
        "    from utils.util_att   import update_pre_activated_actions, \\\n",
        "                                 update_model, \\\n",
        "                                 update_gradient_matrix, \\\n",
        "                                 initialize_pre_activated_actions, \\\n",
        "                                 sequentialize, \\\n",
        "                                 obtain_tensor_from_list, \\\n",
        "                                 obtain_TD_error, \\\n",
        "                                 save_performance_to_csv\n",
        "else:\n",
        "    from models.model_rnn import build_model\n",
        "    from utils.util_rnn   import update_pre_activated_actions, \\\n",
        "                                 update_model, \\\n",
        "                                 update_gradient_matrix, \\\n",
        "                                 initialize_pre_activated_actions, \\\n",
        "                                 sequentialize, \\\n",
        "                                 obtain_tensor_from_list, \\\n",
        "                                 obtain_TD_error, \\\n",
        "                                 save_performance_to_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "    for i in range(len(model_list)):\n",
        "        model_list[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n",
        "gradient_matrix_list = [''] * len(model_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulHSk5_SxeG"
      },
      "source": [
        "Creating intial gradient matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "prev_model_list = copy.deepcopy(model_list)\n",
        "\n",
        "prev_gradient_matrix_list = []\n",
        "for model in model_list:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_list.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "df8f8daf-8e05-41fb-dd81-5dc6983bd6ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100000 [00:00<?, ?it/s]c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1142: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:1424.)\n",
            "  result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/100000 [34:49<58039:15:44, 2089.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/100000 [1:08:21<56775:14:27, 2043.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/100000 [1:41:50<56324:48:26, 2027.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/100000 [2:34:21<68646:30:33, 2471.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5: Summed_Reward = 50.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/100000 [3:29:54<77273:53:06, 2782.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 6: Summed_Reward = 41.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 6/100000 [3:59:54<68006:13:41, 2448.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 7: Summed_Reward = 166.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/100000 [4:25:57<59958:27:06, 2158.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 8: Summed_Reward = 209.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 8/100000 [5:04:23<61263:14:32, 2205.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 9: Summed_Reward = 204.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 9/100000 [6:14:26<78601:54:23, 2829.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10: Summed_Reward = 297.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 10/100000 [7:07:33<81669:52:30, 2940.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 11: Summed_Reward = 298.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 11/100000 [7:51:34<79118:59:41, 2848.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 12: Summed_Reward = 341.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 12/100000 [8:33:17<76197:15:15, 2743.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 13: Summed_Reward = 360.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env           = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state         = env.reset()\n",
        "    summed_reward = 0\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state)\n",
        "    short_term_state_list.append(state[0])\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # initializing and updating actions\n",
        "        state                 = torch.tensor(state, dtype=torch.float)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float)\n",
        "        pre_activated_actions = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_list,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta,\n",
        "                                                             device)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "        action           = vectorizing_action(action_size, action_argmax)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_argmax)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state)\n",
        "        short_term_state_list.append(state[0])\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode+1, summed_reward])\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer \n",
        "    short_term_sequentialized_state_list, \\\n",
        "    short_term_sequentialized_actions_list, \\\n",
        "    short_term_sequentialized_reward_list, \\\n",
        "    short_term_sequentialized_next_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size )\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # saving short term experience replay buffer to long term experience replay buffer\n",
        "    short_term_sequentialized_state_tensor,\\\n",
        "    short_term_sequentialized_actions_tensor,\\\n",
        "    short_term_sequentialized_reward_tensor,\\\n",
        "    short_term_sequentialized_next_state_tensor,\\\n",
        "    short_term_sequentialized_padding_mask = obtain_tensor_from_list(short_term_sequentialized_state_list,\n",
        "                                                                     short_term_sequentialized_actions_list,\n",
        "                                                                     short_term_sequentialized_reward_list,\n",
        "                                                                     short_term_sequentialized_next_state_list,\n",
        "                                                                     time_size,\n",
        "                                                                     mask_value,\n",
        "                                                                     num_heads,\n",
        "                                                                     device) \n",
        "    if training_episode==0:\n",
        "        long_term_sequentialized_state_tensor      = copy.deepcopy(short_term_sequentialized_state_tensor)\n",
        "        long_term_sequentialized_actions_tensor    = copy.deepcopy(short_term_sequentialized_actions_tensor)\n",
        "        long_term_sequentialized_reward_tensor     = copy.deepcopy(short_term_sequentialized_reward_tensor)\n",
        "        long_term_sequentialized_next_state_tensor = copy.deepcopy(short_term_sequentialized_next_state_tensor)\n",
        "        long_term_sequentialized_padding_mask      = copy.deepcopy(short_term_sequentialized_padding_mask)\n",
        "    else:\n",
        "        long_term_sequentialized_state_tensor      = torch.cat((long_term_sequentialized_state_tensor     , short_term_sequentialized_state_tensor     ), dim=0)\n",
        "        long_term_sequentialized_actions_tensor    = torch.cat((long_term_sequentialized_actions_tensor   , short_term_sequentialized_actions_tensor   ), dim=0)\n",
        "        long_term_sequentialized_reward_tensor     = torch.cat((long_term_sequentialized_reward_tensor    , short_term_sequentialized_reward_tensor    ), dim=0)\n",
        "        long_term_sequentialized_next_state_tensor = torch.cat((long_term_sequentialized_next_state_tensor, short_term_sequentialized_next_state_tensor), dim=0)\n",
        "        long_term_sequentialized_padding_mask      = torch.cat((long_term_sequentialized_padding_mask     , short_term_sequentialized_padding_mask     ), dim=0)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # creating dataset and data loader\n",
        "        dataset      = TensorDataset(long_term_sequentialized_state_tensor     ,\n",
        "                                     long_term_sequentialized_actions_tensor   ,\n",
        "                                     long_term_sequentialized_reward_tensor    ,\n",
        "                                     long_term_sequentialized_next_state_tensor,\n",
        "                                     long_term_sequentialized_padding_mask     )\n",
        "        data_loader  = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER) and Elastic Weight Control (EWC)\n",
        "        for i, model in enumerate(model_list):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # creating TD error probability\n",
        "                TD_error     = obtain_TD_error(model, data_loader)\n",
        "                TD_error     =(TD_error.cpu().numpy() + PER_epsilon) ** PER_exponent\n",
        "                TD_error_p   = TD_error / np.sum(TD_error)\n",
        "                # creating sub dataset and sub data loader from  TD error probability\n",
        "                index_arry       = np.random.choice(range(len(dataset)), \n",
        "                                                    p=TD_error_p, \n",
        "                                                    size=iteration_for_learning, \n",
        "                                                    replace=True)\n",
        "                index_arry       = np.random.permutation(index_arry)\n",
        "                sub_dataset      = Subset(dataset, index_arry)\n",
        "                sub_data_loader  = DataLoader(sub_dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # training with PER and EWC\n",
        "                model                     = update_model(model,\n",
        "                                                         sub_data_loader,\n",
        "                                                         prev_model_list[i],\n",
        "                                                         prev_gradient_matrix_list[i],\n",
        "                                                         EWC_lambda)\n",
        "                model_list[i]             = model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # obtaining EWC gradient\n",
        "                gradient_matrix           = update_gradient_matrix(model,\n",
        "                                                                   data_loader)\n",
        "                gradient_matrix_list[i]   = gradient_matrix\n",
        "        torch.cuda.synchronize()\n",
        "        prev_model_list           = copy.deepcopy(model_list)\n",
        "        prev_gradient_matrix_list = copy.deepcopy(gradient_matrix_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_list)):\n",
        "            torch.save(model_list[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        shift,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        drop_rate,\n",
        "                        alpha,\n",
        "                        mask_value)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "for i in range(len(model_list)):\n",
        "    model_list[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                 = torch.tensor(state, dtype=torch.float)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float)\n",
        "        pre_activated_actions = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_list,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta,\n",
        "                                                             device)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "\n",
        "        state, reward, done,  info = env.step(action_argmax)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
