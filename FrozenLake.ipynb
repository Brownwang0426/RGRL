{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "3350c060-7302-4eeb-bfc9-f73f03974e0e"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0y9RfWif-v2",
        "outputId": "31b7bb14-f74e-4aaf-fe69-4cebbdb8bbcf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "af231b3d-b797-492f-b1ad-2d5ce1b94ff0"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYykFEEVSxd9"
      },
      "source": [
        "# Class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 h_input_neuron_size,\n",
        "                 hidden_neuron_size,\n",
        "                 input_neuron_size,\n",
        "                 input_sequence_size,\n",
        "                 output_neuron_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 initializer,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 drop_rate,\n",
        "                 alpha,\n",
        "                 mask_value):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.h_input_neuron_size  = h_input_neuron_size\n",
        "        self.hidden_neuron_size   = hidden_neuron_size\n",
        "        self.input_neuron_size    = input_neuron_size\n",
        "        self.input_sequence_size  = input_sequence_size\n",
        "        self.output_neuron_size   = output_neuron_size\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_layers           = num_layers\n",
        "        self.num_heads            = num_heads\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.initializer          = initializer\n",
        "        self.optimizer            = optimizer\n",
        "        self.loss                 = loss\n",
        "        self.drop_rate            = drop_rate\n",
        "        self.alpha                = alpha\n",
        "        self.mask_value           = mask_value\n",
        "\n",
        "        self.bias = False\n",
        "\n",
        "        neural_types = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM\n",
        "        }\n",
        "        self.state_linear_in_0      = nn.Linear(self.h_input_neuron_size, self.hidden_neuron_size , bias=self.bias)\n",
        "        self.state_linear_in_1      = nn.Linear(self.hidden_neuron_size , self.hidden_neuron_size , bias=self.bias)\n",
        "        self.state_linear_out_0     = nn.Linear(self.hidden_neuron_size , self.hidden_neuron_size , bias=self.bias)\n",
        "        self.state_linear_out_1     = nn.Linear(self.hidden_neuron_size , self.h_input_neuron_size, bias=self.bias)\n",
        "        self.recurrent_layer        = neural_types[neural_type.lower()](self.input_neuron_size, self.hidden_neuron_size, num_layers=self.num_layers, batch_first=False, bias=self.bias, dropout=self.drop_rate)\n",
        "        self.reward_linear          = nn.Linear(self.hidden_neuron_size , self.output_neuron_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.initializer  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.optimizer.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(reduction='none'),\n",
        "            'binary_crossentropy': torch.nn.BCELoss(reduction='none')\n",
        "        }\n",
        "        self.loss_function_ = losses[self.loss .lower()]\n",
        "\n",
        "\n",
        "    def forward(self, s, a, padding_mask):\n",
        "\n",
        "        s          = self.state_linear_in_0(s)\n",
        "        s          = self.hidden_activation(s)\n",
        "        s          = self.state_linear_in_1(s)\n",
        "        s          = self.hidden_activation(s)\n",
        "        s          = torch.unsqueeze(s, dim=0).repeat(self.num_layers, 1, 1)\n",
        "\n",
        "        a          = a.permute(1, 0, 2)\n",
        "        lengths    = (a != self.mask_value).any(dim=2).sum(dim=0).cpu().long() # since a is (sequence_length, batch_size, input_size), we should use sum(dim=0)\n",
        "        a          = rnn_utils.pack_padded_sequence(a, lengths, batch_first=False, enforce_sorted=False)\n",
        "        if self.neural_type == 'lstm':\n",
        "            r, s   = self.recurrent_layer(a, (s, s))\n",
        "        else:\n",
        "            r, s   = self.recurrent_layer(a, s)\n",
        "\n",
        "        r, _       = rnn_utils.pad_packed_sequence(r, batch_first=False)\n",
        "        padding    = (0, 0, 0, 0, 0, self.input_sequence_size - r.size(0))\n",
        "        r          = F.pad(r, padding, \"constant\", 0)\n",
        "        r          = r.permute(1, 0, 2)\n",
        "\n",
        "        s          = self.state_linear_out_0(s)\n",
        "        s          = self.hidden_activation(s)\n",
        "        s          = self.state_linear_out_1(s)\n",
        "        s          = self.output_activation(s)\n",
        "\n",
        "        r          = self.reward_linear(r[:,-1])\n",
        "        r          = self.custom_activation(r)\n",
        "        \n",
        "        return r, s\n",
        "\n",
        "\n",
        "    def custom_activation(self, x):\n",
        "        return torch.sigmoid(x + 1.5)\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYsyx03Sxd_"
      },
      "source": [
        "# Function for updating pre-activated actions using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_pre_activated_actions(iteration_for_deducing,\n",
        "                                 model_loader,\n",
        "                                 state,\n",
        "                                 pre_activated_actions,\n",
        "                                 desired_reward,\n",
        "                                 beta):\n",
        "\n",
        "    model_loader_copy = copy.deepcopy(model_loader)\n",
        "\n",
        "    for _ in range(iteration_for_deducing):\n",
        "\n",
        "        model   = random.choice(model_loader_copy)\n",
        "\n",
        "        actions = torch.sigmoid(pre_activated_actions)\n",
        "\n",
        "        model.train()\n",
        "        actions = actions.clone().detach().requires_grad_(True)\n",
        "        if actions.grad is not None:\n",
        "            actions.grad.zero_()\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        loss_function = model.loss_function\n",
        "        output, _     = model(state, actions, padding_mask=None)\n",
        "        total_loss    = loss_function(output, desired_reward)\n",
        "        total_loss.backward() # get grad\n",
        "\n",
        "        pre_activated_actions -= actions.grad * (1 - actions) * actions * beta # update params\n",
        "\n",
        "    return pre_activated_actions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yskW6bb1SxeA"
      },
      "source": [
        "# Function for updating model using error backprop\n",
        "\n",
        "Elastic weight consolidation:\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rLQCQigdSxeA"
      },
      "outputs": [],
      "source": [
        "# traditional EWC\n",
        "def EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix):\n",
        "    model_param      = model.state_dict()\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        diagonal_fisher_matrix = prev_gradient_matrix[name] ** 2\n",
        "        param_diff             = (model_param[name] - prev_model_param[name]) ** 2\n",
        "        loss                  += (diagonal_fisher_matrix * param_diff).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_model(iteration_for_learning,\n",
        "                 model,\n",
        "                 sub_data_loader,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 EWC_lambda):\n",
        "    \n",
        "    for state, actions, reward, next_state, padding_mask in sub_data_loader:\n",
        "\n",
        "        next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function               = model.loss_function\n",
        "        output, output_state        = model(state, actions, padding_mask)\n",
        "        total_loss                  = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "        total_loss                 += EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix)\n",
        "        total_loss.backward()     # get grad\n",
        "\n",
        "        selected_optimizer.step() # update params\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_gradient_matrix(model,\n",
        "                           data_loader):\n",
        "    \n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for state, actions, reward, next_state, padding_mask in data_loader:\n",
        "            \n",
        "        next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function\n",
        "        output, output_state = model(state, actions, padding_mask)\n",
        "        total_loss           = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "        total_loss.backward()        # get grad\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            gradient_matrix[name] += param.grad\n",
        "\n",
        "    gradient_matrix = {name: param / len(data_loader) for name, param in gradient_matrix.items()}\n",
        "\n",
        "    return gradient_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrw4zh7SxeB"
      },
      "source": [
        "# Function for re-initializing action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_pre_activated_actions(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for vectorizing\n",
        "Crucial function regarding how you manipulate or shape your state, action and reward\n",
        "\n",
        "- It's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "- As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def vectorizing_state(state):      # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0      = np.eye(16)[state]\n",
        "    state        = np.atleast_2d(state_0)\n",
        "    return state\n",
        "\n",
        "def vectorizing_action(action_size, action_argmax):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    return np.eye(action_size)[action_argmax]\n",
        "\n",
        "def vectorizing_reward(state, reward, summed_reward, done, reward_size):       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    if done:\n",
        "        if (state == 15):\n",
        "            reward = np.ones(reward_size)\n",
        "        else:\n",
        "            reward = np.zeros(reward_size)\n",
        "    else:\n",
        "        x, y = divmod(state, 4)\n",
        "        distance = np.sqrt((x - 3) ** 2 + (y - 3) ** 2)\n",
        "        max_distance = np.sqrt(3**2 + 3**2)  # 4.24\n",
        "        idx = int(100 * (1 - (distance / max_distance)))\n",
        "        reward = np.zeros(reward_size)\n",
        "        reward[0: idx        ] = 1\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENsHGddSxeC"
      },
      "source": [
        "# Function for sequentializing state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I don't know why the following multi-processing does not work...T_T (I suck) But I kept it just for later investigation.\n",
        "\n",
        "def process_time_size(params):\n",
        "    state_list, action_list, reward_list, time_size, time = params\n",
        "\n",
        "    sequentialized_state_list      = []\n",
        "    sequentialized_action_list     = []\n",
        "    sequentialized_reward_list     = []\n",
        "    sequentialized_next_state_list = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    # time_size_ = time_size\n",
        "    # for i in range(len(reward_list[:])):\n",
        "    #     sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #     sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #     sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #     sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    # a more sophisticated method\n",
        "    for j in range(time_size):\n",
        "        time_size_ = j+1\n",
        "        if time_size_== 1:\n",
        "            for i in range(len(reward_list[:])):\n",
        "                sequentialized_state_list.append(       state_list [ i ] )\n",
        "                sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "                sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "                sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "        else:\n",
        "            for i in range(len(reward_list[:-time_size_+1])):\n",
        "                sequentialized_state_list.append(       state_list [ i ] )\n",
        "                sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "                sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "                sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    return (sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "    # Prepare the parameters for each process\n",
        "    params_list = [(state_list, action_list, reward_list, time_size, time) for time in range(time_size)]\n",
        "\n",
        "    # Use multiprocessing Pool to process chunks in parallel\n",
        "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
        "        results = pool.map(process_time_size, params_list)\n",
        "\n",
        "    # Aggregate results\n",
        "    sequentialized_state_list = []\n",
        "    sequentialized_action_list = []\n",
        "    sequentialized_reward_list = []\n",
        "    sequentialized_next_state_list = []\n",
        "\n",
        "    for result in results:\n",
        "        s_states, s_actions, s_rewards, s_next_states = result\n",
        "        sequentialized_state_list.extend(s_states)\n",
        "        sequentialized_action_list.extend(s_actions)\n",
        "        sequentialized_reward_list.extend(s_rewards)\n",
        "        sequentialized_next_state_list.extend(s_next_states)\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "\n",
        "    sequentialized_state_list       = []\n",
        "    sequentialized_action_list      = []\n",
        "    sequentialized_reward_list      = []\n",
        "    sequentialized_next_state_list  = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    # time_size_ = time_size\n",
        "    # for i in range(len(reward_list[:])):\n",
        "    #     sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #     sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #     sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #     sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    # a more sophisticated method\n",
        "    for j in range(time_size):\n",
        "        time_size_ = j+1\n",
        "        if time_size_== 1:\n",
        "            for i in range(len(reward_list[:])):\n",
        "                sequentialized_state_list.append(       state_list [ i ] )\n",
        "                sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "                sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "                sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "        else:\n",
        "            for i in range(len(reward_list[:-time_size_+1])):\n",
        "                sequentialized_state_list.append(       state_list [ i ] )\n",
        "                sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "                sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "                sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTosZ2u01xa"
      },
      "source": [
        "# Function for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1zJs9Dgv01xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_from_list(sequentialized_state_list,\n",
        "                            sequentialized_actions_list,\n",
        "                            sequentialized_reward_list,\n",
        "                            sequentialized_next_state_list,\n",
        "                            device,\n",
        "                            time_size,\n",
        "                            mask_value,\n",
        "                            num_heads):\n",
        "\n",
        "    # Convert lists to tensors directly on the desired device and data type\n",
        "    state_tensor = torch.tensor(np.array(sequentialized_state_list), dtype=torch.float).to(device)\n",
        "    reward_tensor = torch.tensor(np.array(sequentialized_reward_list), dtype=torch.float).to(device)\n",
        "    next_state_tensor = torch.tensor(np.array(sequentialized_next_state_list), dtype=torch.float).to(device)\n",
        "\n",
        "    # Pad and stack actions_tensor efficiently\n",
        "    actions_list = []\n",
        "    for arr in sequentialized_actions_list:\n",
        "        tensor_arr = torch.tensor(np.array(arr), dtype=torch.float).to(device)\n",
        "        # Pad tensor only once per tensor\n",
        "        if tensor_arr.size(0) < time_size:\n",
        "            padded_arr = F.pad(tensor_arr,\n",
        "                               (0, 0, 0, time_size - tensor_arr.size(0)),\n",
        "                               mode='constant',\n",
        "                               value=mask_value)\n",
        "        else:\n",
        "            padded_arr = tensor_arr\n",
        "        actions_list.append(padded_arr)\n",
        "    actions_tensor = torch.stack(actions_list).to(device)\n",
        "\n",
        "    # Compute row_mask and padding_mask efficiently\n",
        "    row_mask = (actions_tensor == mask_value).all(dim=-1)\n",
        "    padding_mask = row_mask.to(dtype=torch.bool)\n",
        "    padding_mask = padding_mask.to(device)\n",
        "\n",
        "    return state_tensor, actions_tensor, reward_tensor, next_state_tensor, padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def obtain_TD_error(model,\n",
        "                    train_loader_,\n",
        "                    device):\n",
        "\n",
        "\n",
        "    for state, actions, reward, next_state, padding_mask in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function_\n",
        "        output, _            = model(state, actions, padding_mask)\n",
        "        total_loss           = loss_function(output, reward).detach().to(device)\n",
        "        total_loss           = torch.sum(torch.abs(total_loss), axis=1)\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = 'FrozenLake-v1'              # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "max_steps_for_each_episode = 25          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 10                # choose the size of the neural ensemble (Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️)\n",
        "state_size =  16                  # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 4                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 8                     # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'gru'               # choose your neural type: [rnn gru lstm] [att]\n",
        "num_layers = 2                    # choose the number of layers for your rnn or attention: [1 2 3 4 etc.]\n",
        "num_heads = None                  # choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu leaky_relu sigmoid tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu leaky_relu sigmoid tanh]\n",
        "init = \"random_normal\"            # choose initialization method: [random_normal random_uniform xavier_normal xavier_uniform glorot_normal glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam sgd rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error binary_crossentropy]\n",
        "drop_rate = 0.0001                # choose your drop rate (Reminder: change this value to see the impact of drop-out ◀️◀️◀️)\n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "iteration_for_learning = 10000    # choose learning iteration for nn weights (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "mask_value = sys.maxsize          # mask value\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1                       # gaussian noise\n",
        "noise_r = 0.1                     # smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "beta = 0.1                        # updating rate for input actions\n",
        "iteration_for_deducing =  100     # updating iteration for input actions (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 10     # batch size for batch offline learning (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "PER_exponent = 0                         # prioritized_experience_replay (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "EWC_lambda = 1                           # elastic weight control lambda (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n",
        "\n",
        "gradient_matrix_loader = [''] * len(model_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulHSk5_SxeG"
      },
      "source": [
        "Creating intial gradient matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "df8f8daf-8e05-41fb-dd81-5dc6983bd6ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env           = gym.make(game_name, is_slippery=False, map_name=\"4x4\")\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state         = env.reset()\n",
        "    summed_reward = 0\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state)\n",
        "    short_term_state_list.append(state[0])\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # initializing and updating actions\n",
        "        state                 = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_loader,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "        action           = vectorizing_action(action_size, action_argmax)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_argmax)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state)\n",
        "        short_term_state_list.append(state[0])\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode+1, summed_reward])\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer \n",
        "    short_term_sequentialized_state_list, \\\n",
        "    short_term_sequentialized_actions_list, \\\n",
        "    short_term_sequentialized_reward_list, \\\n",
        "    short_term_sequentialized_next_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size )\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # saving short term experience replay buffer to long term experience replay buffer\n",
        "    short_term_sequentialized_state_tensor,\\\n",
        "    short_term_sequentialized_actions_tensor,\\\n",
        "    short_term_sequentialized_reward_tensor,\\\n",
        "    short_term_sequentialized_next_state_tensor,\\\n",
        "    short_term_sequentialized_padding_mask = obtain_tensor_from_list(short_term_sequentialized_state_list,\n",
        "                                                                     short_term_sequentialized_actions_list,\n",
        "                                                                     short_term_sequentialized_reward_list,\n",
        "                                                                     short_term_sequentialized_next_state_list,\n",
        "                                                                     device,\n",
        "                                                                     time_size,\n",
        "                                                                     mask_value,\n",
        "                                                                     num_heads)\n",
        "    if training_episode==0:\n",
        "        long_term_sequentialized_state_tensor      = copy.deepcopy(short_term_sequentialized_state_tensor)\n",
        "        long_term_sequentialized_actions_tensor    = copy.deepcopy(short_term_sequentialized_actions_tensor)\n",
        "        long_term_sequentialized_reward_tensor     = copy.deepcopy(short_term_sequentialized_reward_tensor)\n",
        "        long_term_sequentialized_next_state_tensor = copy.deepcopy(short_term_sequentialized_next_state_tensor)\n",
        "        long_term_sequentialized_padding_mask      = copy.deepcopy(short_term_sequentialized_padding_mask)\n",
        "    else:\n",
        "        long_term_sequentialized_state_tensor      = torch.cat((long_term_sequentialized_state_tensor     , short_term_sequentialized_state_tensor     ), dim=0)\n",
        "        long_term_sequentialized_actions_tensor    = torch.cat((long_term_sequentialized_actions_tensor   , short_term_sequentialized_actions_tensor   ), dim=0)\n",
        "        long_term_sequentialized_reward_tensor     = torch.cat((long_term_sequentialized_reward_tensor    , short_term_sequentialized_reward_tensor    ), dim=0)\n",
        "        long_term_sequentialized_next_state_tensor = torch.cat((long_term_sequentialized_next_state_tensor, short_term_sequentialized_next_state_tensor), dim=0)\n",
        "        long_term_sequentialized_padding_mask      = torch.cat((long_term_sequentialized_padding_mask     , short_term_sequentialized_padding_mask     ), dim=0)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # creating dataset and data loader\n",
        "        dataset      = TensorDataset(long_term_sequentialized_state_tensor     ,\n",
        "                                     long_term_sequentialized_actions_tensor   ,\n",
        "                                     long_term_sequentialized_reward_tensor    ,\n",
        "                                     long_term_sequentialized_next_state_tensor,\n",
        "                                     long_term_sequentialized_padding_mask     )\n",
        "        data_loader  = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER) and Elastic Weight Control (EWC)\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # creating TD error probability\n",
        "                TD_error     = obtain_TD_error(model, data_loader, device)\n",
        "                TD_error     = TD_error.cpu().numpy() ** PER_exponent\n",
        "                TD_error_p   = TD_error / np.sum(TD_error)\n",
        "\n",
        "                # creating sub dataset and sub data loader from  TD error probability\n",
        "                index_list       = np.random.choice(range(len(dataset)), \n",
        "                                                    p = TD_error_p, \n",
        "                                                    size = iteration_for_learning, \n",
        "                                                    replace=True)\n",
        "                sub_dataset      = Subset(dataset, index_list)\n",
        "                sub_data_loader  = DataLoader(sub_dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # training with PER and EWC\n",
        "                model                     = update_model(iteration_for_learning,\n",
        "                                                         model,\n",
        "                                                         sub_data_loader,\n",
        "                                                         prev_model_loader[i],\n",
        "                                                         prev_gradient_matrix_loader[i],\n",
        "                                                         EWC_lambda)\n",
        "                model_loader[i]           = model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # obtaining EWC gradient\n",
        "                gradient_matrix           = update_gradient_matrix(model,\n",
        "                                                                   data_loader)\n",
        "                gradient_matrix_loader[i] = gradient_matrix\n",
        "        torch.cuda.synchronize()\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_gradient_matrix_loader = copy.deepcopy(gradient_matrix_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        drop_rate,\n",
        "                        alpha,\n",
        "                        mask_value)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, is_slippery=False, map_name=\"4x4\", render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name, is_slippery=False, map_name=\"4x4\")\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                 = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_loader,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "\n",
        "        state, reward, done,  info = env.step(action_argmax)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
