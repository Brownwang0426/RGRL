{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AssistMoli/Genrl/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0y9RfWif-v2"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit\n",
        "import gym\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "\n",
        "import multiprocessing\n",
        "import time\n",
        "import csv\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229QV7Hhf-v3",
        "outputId": "83c55d03-f498-4b65-965a-bd08dccb8205"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA GeForce RTX 4090\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\") \n",
        "    print('using cpu...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "\n",
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_neuron_size_,\n",
        "                 hidden_neuron_size,\n",
        "                 input_neuron_size,\n",
        "                 input_sequence_size,\n",
        "                 output_neuron_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 initializer,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 alpha,\n",
        "                 mask_value):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.input_neuron_size_   = int(input_neuron_size_)\n",
        "        self.hidden_neuron_size   = int(hidden_neuron_size)\n",
        "        self.input_neuron_size    = int(input_neuron_size)\n",
        "        self.input_sequence_size  = int(input_sequence_size)\n",
        "        self.output_neuron_size   = int(output_neuron_size)\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_heads            = num_heads\n",
        "\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.initializer          = initializer\n",
        "        self.optimizer            = optimizer\n",
        "        self.loss                 = loss\n",
        "        self.alpha                = alpha\n",
        "        self.mask_value           = mask_value\n",
        "\n",
        "        self.bias = False\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        neural_types = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM\n",
        "        }\n",
        "\n",
        "        self.fully_connected_layer_in_0      = nn.Linear(self.input_neuron_size_, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_in_1      = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_out_0     = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_out_1     = nn.Linear(self.hidden_neuron_size, self.input_neuron_size_, bias=self.bias)\n",
        "\n",
        "        self.recurrent_layer_0               = neural_types[neural_type.lower()](self.input_neuron_size, self.hidden_neuron_size, num_layers=self.num_layers, batch_first=False, bias=self.bias)\n",
        "\n",
        "        self.fully_connected_layer_7         = nn.Linear(self.hidden_neuron_size * self.input_sequence_size, self.output_neuron_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.initializer  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.optimizer.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "    def forward(self, initial_hidden, x, padding_mask):\n",
        "\n",
        "        h  = self.fully_connected_layer_in_0(initial_hidden)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_in_1(h)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = torch.unsqueeze(h, dim=0).repeat(self.num_layers, 1, 1)\n",
        "\n",
        "        out          = x.permute(1, 0, 2)\n",
        "        lengths      = (out != self.mask_value).any(dim=2).sum(dim=0).cpu().long() # since x is (sequence_length, batch_size, input_size), we should use sum(dim=0)\n",
        "        out          = rnn_utils.pack_padded_sequence(out, lengths, batch_first=False, enforce_sorted=False)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        if self.neural_type == 'lstm':\n",
        "            out, h     = self.recurrent_layer_0(out, (h, h))\n",
        "            h          = h[0] \n",
        "        else:\n",
        "            out, h     = self.recurrent_layer_0(out, h)\n",
        "            h          = h \n",
        "\n",
        "        out, _     = rnn_utils.pad_packed_sequence(out, batch_first=False)\n",
        "        padding    = (0, 0, 0, 0, 0, self.input_sequence_size - out.size(0))\n",
        "        out        = F.pad(out, padding, \"constant\", 0)\n",
        "        out        = out.permute(1, 0, 2)\n",
        "\n",
        "        # h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_out_0(h)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_out_1(h)\n",
        "        h  = self.output_activation(h)\n",
        "\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "\n",
        "        out = self.fully_connected_layer_7(out)\n",
        "        out = self.output_activation(out)\n",
        "\n",
        "        return out, h\n",
        "\n",
        "\n",
        "    def custom_activation(self, x):\n",
        "        return torch.sigmoid(x - 1.5)\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for updating input value using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_actions_value(epoch_for_deducing,\n",
        "            model_loader,\n",
        "            desired_reward,\n",
        "            state,\n",
        "            actions_value,\n",
        "            beta):\n",
        "\n",
        "    model_loader_ = copy.deepcopy(model_loader)\n",
        "\n",
        "    for epoch in range(epoch_for_deducing):\n",
        "\n",
        "        random.shuffle(model_loader_)\n",
        "\n",
        "        for model in model_loader_:\n",
        "\n",
        "            action = torch.sigmoid(actions_value)\n",
        "\n",
        "            action = action.clone().detach().requires_grad_(True)\n",
        "            if action.grad is not None:\n",
        "                action.grad.zero_()\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, _ = model(state, action, padding_mask=None)\n",
        "            total_loss = loss_function(output, desired_reward)\n",
        "\n",
        "            total_loss.backward() # Error Backpropagation\n",
        "            actions_value -= action.grad * (1 - action) * action * beta # Update Input Data\n",
        "\n",
        "    return actions_value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for updating weight matrices using error backprop\n",
        "\n",
        "Elastic weight consolidation:\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traditional EWC\n",
        "def EWC_loss( EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size):\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        loss += ( ((prev_gradient_matrix[name])**2) * ((param - prev_model_param[name])**2)     ).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "def update_model(batch_size,\n",
        "                 epoch_for_learning,\n",
        "                 model,\n",
        "                 train_loader,\n",
        "                 dataset,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 prev_train_loader_size,\n",
        "                 EWC_lambda):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # retrieving present_gradient_matrix\n",
        "    present_model = copy.deepcopy(model)\n",
        "    present_gradient_matrix =  {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            for name, param in model.named_parameters():\n",
        "                present_gradient_matrix[name] += param.grad\n",
        "\n",
        "    present_gradient_matrix = {name: param / len(train_loader) for name, param in present_gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epoch_for_learning):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward)  + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss += EWC_loss(EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            selected_optimizer.step() # Update Model Weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # training and updating present_gradient_matrix\n",
        "    updated_present_gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward)  + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            for name, param in model.named_parameters():\n",
        "                updated_present_gradient_matrix[name] += param.grad \n",
        "\n",
        "    updated_present_gradient_matrix = {name: param / len(train_loader) for name, param in updated_present_gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    present_train_loader_size = len(train_loader) + prev_train_loader_size\n",
        "\n",
        "    return model, present_gradient_matrix, present_train_loader_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for re-initialize action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_actions_value(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for sequentialize state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9qGocnYQJ4FO"
      },
      "outputs": [],
      "source": [
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "\n",
        "    sequentialized_state_list  = []\n",
        "    sequentialized_action_list = []\n",
        "    sequentialized_reward_list = []\n",
        "    sequentialized_next_state_list  = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    time_size_ = time_size \n",
        "    for i in range(len(action_list)):\n",
        "        sequentialized_state_list.append( state_list[i ] )\n",
        "        sequentialized_action_list.append( action_list[i:i+time_size_]  )\n",
        "        sequentialized_reward_list.append( reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "        sequentialized_next_state_list.append(  state_list[ i + len(action_list[i:i+time_size_]) ]  )\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1pAeDkANCOE0"
      },
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for vectorizing\n",
        "Crucial function regarding how you manipulate or shape your state, action and reward\n",
        "\n",
        "- It's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "- As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def vectorizing_state(state):      # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0 = quantifying(100, -2.5, 0.050, state[0])     \n",
        "    state_1 = quantifying(100, -3.75, 0.075, state[1])    \n",
        "    state_2 = quantifying(100, -0.375, 0.0075, state[2])  \n",
        "    state_3 = quantifying(100, -3.75, 0.075, state[3])    \n",
        "    state_4 = quantifying(100, 0, 10, 0)                  \n",
        "    state   = np.atleast_2d(np.concatenate((state_0, state_1, state_2, state_3, state_4)))   \n",
        "    return state\n",
        "\n",
        "def vectorizing_action(action_size, action_arg):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    return np.eye(action_size)[action_arg]\n",
        "\n",
        "def vectorizing_reward(state, reward, summed_reward, done, reward_size):       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    if done:                                      \n",
        "        reward = np.zeros(reward_size)             \n",
        "    else:                                          \n",
        "        reward = np.ones(reward_size)              \n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'                # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "max_steps_for_each_episode = 2000        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "state_size =  500                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "hidden_size = 100                        # Reminder: change this for your specific task ⚠️⚠️⚠️ (should be dividable by num_heads below)\n",
        "action_size = 2                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "time_size = 15                           # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "chunk_size = 15                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "reward_size = 100                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "ensemble_size = 10                       # Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️\n",
        "neural_type = 'gru'                      # rnn gru lstm\n",
        "num_layers = 2                           # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "num_heads  = 10                          # should be able to divide hidden_size\n",
        "hidden_activation = 'tanh'               # relu leaky_relu sigmoid tanh\n",
        "output_activation = 'sigmoid'            # relu leaky_relu sigmoid tanh\n",
        "init = \"random_normal\"                   # random_normal random_uniform xavier_normal xavier_uniform  glorot_normal  glorot_uniform\n",
        "loss = 'mean_squared_error'              # mean_squared_error  binary_crossentropy\n",
        "opti = 'sgd'                             # adam sgd rmsprop\n",
        "alpha = 0.1\n",
        "epoch_for_learning = 10\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "noise_t = 1\n",
        "noise_r = 0.1\n",
        "beta = 0.1\n",
        "epoch_for_deducing =  int(100/ensemble_size)\n",
        "\n",
        "\n",
        "episode_for_training             = 100000\n",
        "replay_range                     = 2                     # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "interval_for_initiating_learning = 50                    # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "EWC_lambda = 0                                           # Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️\n",
        "\n",
        "\n",
        "episode_for_testing = 100                # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "render_for_human = False                 # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mask_value = sys.maxsize\n",
        "load_pre_model = False\n",
        "suffix                      = f\"ensemble={ensemble_size:05d}_learn={epoch_for_learning:05d}_interval={interval_for_initiating_learning:05d}_deduce={epoch_for_deducing:05d}\"\n",
        "directory                   = f'/content/Genrl/{game_name}/'\n",
        "model_directory             = f'/content/Genrl/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/Genrl/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating intial gradient matrices "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "storing previous models\n",
        "\"\"\"\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "prev_train_loader_size = 1\n",
        "\n",
        "\"\"\"\n",
        "calculating gradient matrix\n",
        "\"\"\"\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "82616cd1-01db-49b6-a643-4a7ce74ad7cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "  0%|          | 0/100000 [00:00<?, ?it/s]C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:1001: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
            "  result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "  0%|          | 1/100000 [00:05<139:06:44,  5.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/100000 [00:11<156:02:42,  5.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/100000 [00:16<160:00:24,  5.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/100000 [00:21<145:15:05,  5.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/100000 [00:26<143:38:18,  5.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 6/100000 [00:33<160:41:33,  5.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 6: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/100000 [00:37<141:05:07,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 7: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 8/100000 [00:41<131:35:29,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 8: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 9/100000 [00:46<135:02:11,  4.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 9: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 10/100000 [00:49<121:32:48,  4.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 11/100000 [01:04<211:08:52,  7.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 11: Summed_Reward = 43.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 12/100000 [01:15<241:58:45,  8.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 12: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 13/100000 [01:23<233:57:37,  8.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 13: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 14/100000 [01:27<200:47:32,  7.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 14: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 15/100000 [01:31<173:38:19,  6.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 15: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 16/100000 [01:34<145:29:23,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 16: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 17/100000 [01:43<171:35:56,  6.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 17: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 18/100000 [01:48<166:37:36,  6.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 18: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 19/100000 [01:54<161:51:03,  5.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 20/100000 [02:00<164:14:44,  5.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 21/100000 [02:05<159:02:05,  5.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 21: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 22/100000 [02:10<154:41:31,  5.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 22: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 23/100000 [02:18<170:24:19,  6.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 23: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 24/100000 [02:22<158:36:16,  5.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 24: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 25/100000 [02:26<144:29:37,  5.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 25: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 26/100000 [02:30<133:35:22,  4.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 26: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 27/100000 [02:35<134:47:42,  4.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 27: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 28/100000 [02:48<200:44:04,  7.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 28: Summed_Reward = 46.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 29/100000 [02:57<213:48:37,  7.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 29: Summed_Reward = 31.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 30/100000 [03:01<180:14:51,  6.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 30: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 31/100000 [03:05<162:15:09,  5.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 31: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 32/100000 [03:10<158:31:26,  5.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 32: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 33/100000 [03:21<201:53:51,  7.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 33: Summed_Reward = 31.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 34/100000 [03:29<208:33:06,  7.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 34: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 35/100000 [03:49<309:35:00, 11.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 35: Summed_Reward = 56.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 36/100000 [03:54<263:24:22,  9.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 36: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 37/100000 [03:59<220:15:00,  7.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 37: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 38/100000 [04:10<248:18:11,  8.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 38: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 39/100000 [04:22<275:51:38,  9.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 39: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 40/100000 [04:35<299:04:19, 10.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 40: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 41/100000 [04:40<251:13:07,  9.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 41: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 42/100000 [04:44<205:58:30,  7.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 42: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 43/100000 [05:07<335:33:29, 12.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 43: Summed_Reward = 68.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 44/100000 [05:11<273:22:05,  9.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 44: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 45/100000 [05:23<289:00:35, 10.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 45: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 46/100000 [05:28<240:35:58,  8.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 46: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 47/100000 [05:34<222:25:35,  8.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 47: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 48/100000 [05:46<256:29:00,  9.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 48: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 49/100000 [06:04<323:56:11, 11.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 49: Summed_Reward = 59.0\n",
            "Episode 50: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16512\\918731856.py:107: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  action_tensor     = torch.stack( [F.pad(torch.tensor(arr),\n",
            "  0%|          | 51/100000 [16:30<3856:30:36, 138.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 51: Summed_Reward = 52.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 52/100000 [16:34<2734:39:17, 98.50s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 52: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 53/100000 [16:44<1992:06:50, 71.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 53: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 54/100000 [16:50<1448:19:28, 52.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 54: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 55/100000 [16:54<1047:16:12, 37.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 55: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 56/100000 [16:57<756:43:09, 27.26s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 56: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 57/100000 [17:02<569:38:05, 20.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 57: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 58/100000 [17:09<461:53:50, 16.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 58: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 59/100000 [17:15<368:00:42, 13.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 59: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 60/100000 [17:18<283:25:59, 10.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 60: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 61/100000 [17:26<265:58:41,  9.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 61: Summed_Reward = 28.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 62/100000 [17:30<220:39:26,  7.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 62: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 63/100000 [17:40<234:21:49,  8.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 63: Summed_Reward = 34.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 64/100000 [18:02<346:18:14, 12.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 64: Summed_Reward = 78.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 65/100000 [18:15<353:01:53, 12.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 65: Summed_Reward = 47.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 66/100000 [18:23<312:09:05, 11.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 66: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 67/100000 [18:35<322:07:53, 11.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 67: Summed_Reward = 44.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 68/100000 [18:52<363:58:52, 13.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 68: Summed_Reward = 53.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 69/100000 [18:56<289:20:08, 10.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 69: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 70/100000 [19:02<256:22:48,  9.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 70: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 71/100000 [19:06<209:43:25,  7.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 71: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 72/100000 [19:09<172:29:23,  6.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 72: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 73/100000 [19:22<228:32:55,  8.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 73: Summed_Reward = 45.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 74/100000 [19:30<224:41:28,  8.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 74: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 75/100000 [19:35<200:08:03,  7.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 75: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 76/100000 [19:47<239:37:42,  8.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 76: Summed_Reward = 38.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 77/100000 [19:51<200:55:09,  7.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 77: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 78/100000 [19:57<191:02:29,  6.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 78: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 79/100000 [20:04<195:10:17,  7.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 79: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 80/100000 [20:12<203:57:20,  7.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 80: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 81/100000 [20:23<234:39:29,  8.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 81: Summed_Reward = 38.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 82/100000 [20:35<257:59:53,  9.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 82: Summed_Reward = 40.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 83/100000 [20:39<215:51:58,  7.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 83: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 84/100000 [20:44<193:41:56,  6.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 84: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 85/100000 [20:47<161:35:05,  5.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 85: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 86/100000 [20:53<158:20:33,  5.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 86: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 87/100000 [20:56<141:42:45,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 87: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 88/100000 [21:01<141:55:44,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 88: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 89/100000 [21:09<164:35:52,  5.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 89: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 90/100000 [21:16<173:05:46,  6.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 90: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 91/100000 [21:21<157:12:12,  5.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 91: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 92/100000 [21:28<172:00:43,  6.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 92: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 93/100000 [21:35<175:41:47,  6.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 93: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 94/100000 [21:37<143:59:32,  5.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 94: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 95/100000 [21:44<158:24:43,  5.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 95: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 96/100000 [21:49<153:06:13,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 96: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 97/100000 [22:06<243:02:53,  8.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 97: Summed_Reward = 57.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 98/100000 [22:08<193:26:25,  6.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 98: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 99/100000 [22:14<185:22:19,  6.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 99: Summed_Reward = 21.0\n",
            "Episode 100: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 101/100000 [46:20<8533:48:48, 307.53s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 101: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 102/100000 [46:39<6133:15:39, 221.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 102: Summed_Reward = 57.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 103/100000 [47:01<4479:33:25, 161.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 103: Summed_Reward = 64.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 104/100000 [47:07<3188:04:50, 114.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 104: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 105/100000 [47:24<2366:41:42, 85.29s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 105: Summed_Reward = 44.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 106/100000 [47:31<1714:25:07, 61.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 106: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 107/100000 [47:35<1236:53:46, 44.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 107: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 108/100000 [47:41<912:24:04, 32.88s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 108: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 109/100000 [47:50<718:57:14, 25.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 109: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 110/100000 [48:12<682:15:36, 24.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 110: Summed_Reward = 64.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 111/100000 [48:40<712:37:20, 25.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 111: Summed_Reward = 84.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 112/100000 [49:11<754:52:07, 27.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 112: Summed_Reward = 88.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 113/100000 [49:21<613:30:18, 22.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 113: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 114/100000 [49:33<530:03:30, 19.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 114: Summed_Reward = 34.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 115/100000 [49:48<491:41:51, 17.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 115: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 116/100000 [49:56<411:23:12, 14.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 116: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 117/100000 [50:09<402:42:08, 14.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 117: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 118/100000 [50:19<363:53:04, 13.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 118: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 119/100000 [50:31<356:19:17, 12.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 119: Summed_Reward = 36.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 120/100000 [50:45<363:08:40, 13.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 120: Summed_Reward = 38.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 121/100000 [51:09<453:22:15, 16.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 121: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 122/100000 [51:20<408:46:51, 14.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 122: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 123/100000 [51:34<398:53:01, 14.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 123: Summed_Reward = 36.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 124/100000 [52:11<587:20:25, 21.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 124: Summed_Reward = 107.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 125/100000 [52:25<530:16:18, 19.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 125: Summed_Reward = 40.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 126/100000 [52:38<479:47:42, 17.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 126: Summed_Reward = 40.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 127/100000 [52:44<389:18:48, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 127: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 128/100000 [52:53<343:00:55, 12.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 128: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 129/100000 [53:17<442:53:29, 15.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 129: Summed_Reward = 67.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 130/100000 [53:22<348:00:06, 12.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 130: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 131/100000 [53:33<339:42:45, 12.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 131: Summed_Reward = 31.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 132/100000 [53:55<422:13:35, 15.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 132: Summed_Reward = 63.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 133/100000 [54:15<456:42:23, 16.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 133: Summed_Reward = 53.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 134/100000 [54:19<358:34:42, 12.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 134: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 135/100000 [55:06<639:11:37, 23.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 135: Summed_Reward = 138.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 136/100000 [55:09<472:33:27, 17.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 136: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 137/100000 [55:22<434:50:59, 15.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 137: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 138/100000 [55:43<478:13:59, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 138: Summed_Reward = 60.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 139/100000 [55:48<376:33:19, 13.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 139: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 140/100000 [56:07<427:23:42, 15.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 140: Summed_Reward = 56.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 141/100000 [56:23<429:44:44, 15.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 141: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 142/100000 [57:03<632:58:22, 22.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 142: Summed_Reward = 114.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 143/100000 [57:16<554:41:11, 20.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 143: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 144/100000 [57:32<520:08:29, 18.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 144: Summed_Reward = 45.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 145/100000 [57:41<440:48:55, 15.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 145: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 146/100000 [57:46<346:43:13, 12.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 146: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 147/100000 [57:55<322:08:48, 11.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 147: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 148/100000 [58:12<360:27:10, 13.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 148: Summed_Reward = 47.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 149/100000 [58:37<466:02:06, 16.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 149: Summed_Reward = 79.0\n",
            "Episode 150: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 151/100000 [1:32:26<12069:12:19, 435.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 151: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 152/100000 [1:32:45<8606:21:12, 310.30s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 152: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 153/100000 [1:33:22<6335:15:43, 228.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 153: Summed_Reward = 131.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 154/100000 [1:33:34<4532:45:56, 163.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 154: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 155/100000 [1:34:14<3502:58:53, 126.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 155: Summed_Reward = 122.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 156/100000 [1:34:24<2537:40:18, 91.50s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 156: Summed_Reward = 37.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 157/100000 [1:34:50<1993:44:08, 71.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 157: Summed_Reward = 89.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 158/100000 [1:35:12<1575:23:54, 56.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 158: Summed_Reward = 75.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 159/100000 [1:35:32<1270:58:55, 45.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 159: Summed_Reward = 69.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 160/100000 [1:35:53<1063:11:28, 38.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 160: Summed_Reward = 73.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 161/100000 [1:36:15<934:23:56, 33.69s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 161: Summed_Reward = 80.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 162/100000 [1:36:24<726:59:18, 26.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 162: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 163/100000 [1:36:32<574:47:20, 20.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 163: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 164/100000 [1:37:16<770:24:39, 27.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 164: Summed_Reward = 149.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 165/100000 [1:37:26<618:28:12, 22.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 165: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 166/100000 [1:37:38<531:11:54, 19.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 166: Summed_Reward = 38.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 167/100000 [1:38:33<830:16:06, 29.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 167: Summed_Reward = 199.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 168/100000 [1:39:06<854:01:17, 30.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 168: Summed_Reward = 115.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 169/100000 [1:39:13<661:56:10, 23.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 169: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 170/100000 [1:39:24<550:25:03, 19.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 170: Summed_Reward = 36.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 171/100000 [1:39:58<673:08:14, 24.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 171: Summed_Reward = 122.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 172/100000 [1:40:28<715:40:45, 25.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 172: Summed_Reward = 108.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 173/100000 [1:40:59<758:34:36, 27.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 173: Summed_Reward = 107.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 174/100000 [1:41:12<639:10:02, 23.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 174: Summed_Reward = 48.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 175/100000 [1:41:56<812:44:38, 29.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 175: Summed_Reward = 161.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 176/100000 [1:42:29<848:25:40, 30.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 176: Summed_Reward = 121.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 177/100000 [1:42:59<844:01:02, 30.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 177: Summed_Reward = 102.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 178/100000 [1:43:30<844:24:44, 30.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 178: Summed_Reward = 110.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 179/100000 [1:43:55<802:24:13, 28.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 179: Summed_Reward = 75.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 180/100000 [1:44:01<610:06:35, 22.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 180: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 181/100000 [1:44:29<663:07:40, 23.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 181: Summed_Reward = 101.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 182/100000 [1:45:03<741:43:58, 26.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 182: Summed_Reward = 114.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 183/100000 [1:45:13<605:46:34, 21.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 183: Summed_Reward = 36.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 184/100000 [1:45:53<753:45:41, 27.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 184: Summed_Reward = 137.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 185/100000 [1:46:29<830:28:35, 29.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 185: Summed_Reward = 127.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 186/100000 [1:46:47<732:30:00, 26.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 186: Summed_Reward = 63.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 187/100000 [1:47:35<908:31:40, 32.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 187: Summed_Reward = 169.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 188/100000 [1:47:43<701:38:04, 25.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 188: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 189/100000 [1:48:25<837:34:11, 30.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 189: Summed_Reward = 147.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 190/100000 [1:49:37<1192:25:44, 43.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 190: Summed_Reward = 255.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 191/100000 [1:50:11<1113:35:50, 40.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 191: Summed_Reward = 118.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 192/100000 [1:50:22<870:17:18, 31.39s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 192: Summed_Reward = 40.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 193/100000 [1:50:34<711:05:53, 25.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 193: Summed_Reward = 43.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 194/100000 [1:51:00<709:17:44, 25.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 194: Summed_Reward = 87.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 195/100000 [1:51:15<620:53:20, 22.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 195: Summed_Reward = 53.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 196/100000 [1:52:32<1082:43:50, 39.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 196: Summed_Reward = 286.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 197/100000 [1:53:22<1170:16:53, 42.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 197: Summed_Reward = 175.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 198/100000 [1:53:44<1002:09:44, 36.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 198: Summed_Reward = 81.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 199/100000 [1:54:56<1300:54:37, 46.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 199: Summed_Reward = 246.0\n",
            "Episode 200: Summed_Reward = 113.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "episode_list = []\n",
        "sequentialized_state_list = []\n",
        "sequentialized_action_list = []\n",
        "sequentialized_reward_list = []\n",
        "sequentialized_next_state_list = []\n",
        "\n",
        "env = gym.make(game_name)   \n",
        "env._max_episode_steps = max_steps_for_each_episode\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "\n",
        "    # Getting state\n",
        "    state = vectorizing_state(state)\n",
        "    state_list.append(state[0])\n",
        "\n",
        "\n",
        "    for _ in range(sys.maxsize):\n",
        "\n",
        "\n",
        "        # Getting action\n",
        "        state         = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        actions_value = initialize_actions_value(init, noise_t, noise_r,(time_size, action_size) )\n",
        "        actions_value = torch.tensor(actions_value, dtype=torch.float).to(device)\n",
        "        actions_value = update_actions_value(epoch_for_deducing,\n",
        "                                           model_loader,\n",
        "                                           desired_reward,\n",
        "                                           state,\n",
        "                                           actions_value,\n",
        "                                           beta)\n",
        "        action_arg    = int(torch.argmax(actions_value[0, 0]))      \n",
        "        action        = vectorizing_action(action_size, action_arg)\n",
        "        action_list.append(action)\n",
        "\n",
        "\n",
        "        # Getting reward\n",
        "        state, reward, done,  info = env.step(action_arg)\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        reward_list.append(reward)\n",
        "\n",
        "\n",
        "        # Getting state\n",
        "        state = vectorizing_state(state)\n",
        "        state_list.append(state[0])\n",
        "\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode+1, summed_reward])\n",
        "            # Save performance log to CSV\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    sequentializing and setting replay range\n",
        "    \"\"\"\n",
        "    sequentialized_state_list_slice, sequentialized_action_list_slice, sequentialized_reward_list_slice, sequentialized_next_state_list_slice = sequentialize(state_list, action_list, reward_list, chunk_size )\n",
        "\n",
        "    episode_list .extend( [ training_episode ] * len(sequentialized_state_list_slice))\n",
        "    sequentialized_state_list       .extend( sequentialized_state_list_slice)\n",
        "    sequentialized_action_list      .extend( sequentialized_action_list_slice)\n",
        "    sequentialized_reward_list      .extend( sequentialized_reward_list_slice)\n",
        "    sequentialized_next_state_list  .extend( sequentialized_next_state_list_slice)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if (training_episode+1) % interval_for_initiating_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        episode_list                   = [i for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_state_list      = [a for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_action_list     = [b for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_reward_list     = [c for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_next_state_list = [d for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        masking and uploading data\n",
        "        \"\"\"\n",
        "\n",
        "        state_tensor      = torch.stack( [torch.tensor(arr) for arr in sequentialized_state_list]               ).float().to(device)\n",
        "        action_tensor     = torch.stack( [F.pad(torch.tensor(arr), \n",
        "                                                pad=(0, 0, 0, time_size - torch.tensor(arr).size(0)), \n",
        "                                                mode='constant', \n",
        "                                                value= mask_value) for arr in sequentialized_action_list]       ).float().to(device)\n",
        "        reward_tensor     = torch.stack( [torch.tensor(arr) for arr in sequentialized_reward_list]              ).float().to(device)\n",
        "        next_state_tensor = torch.stack( [torch.tensor(arr) for arr in sequentialized_next_state_list]          ).float().to(device)\n",
        "\n",
        "        row_mask     = torch.all(action_tensor == mask_value, dim = -1)\n",
        "        padding_mask = torch.zeros_like(action_tensor, dtype = torch.bool)\n",
        "        padding_mask[row_mask] = True\n",
        "        padding_mask = padding_mask.to(device)\n",
        "\n",
        "        dataset     = TensorDataset(state_tensor, action_tensor, reward_tensor, next_state_tensor, padding_mask)\n",
        "        data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        learning\n",
        "        \"\"\"\n",
        "        gradient_matrix_loader_ = []\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "                model, gradient_matrix_, train_loader_size_= update_model(batch_size, epoch_for_learning, model, data_loader, dataset, prev_model_loader[i], prev_gradient_matrix_loader[i], prev_train_loader_size, EWC_lambda)\n",
        "                gradient_matrix_loader_.append(gradient_matrix_)\n",
        "                model_loader[i] = model\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        prev_gradient_matrix_loader = gradient_matrix_loader_\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_train_loader_size      = train_loader_size_\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        saving\n",
        "        \"\"\"\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (tesing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,  \n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        alpha,\n",
        "                        mask_value)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "summed_reward_sum = 0\n",
        "\n",
        "\n",
        "if render_for_human == True:\n",
        "    env = gym.make( game_name, render_mode=\"human\") \n",
        "else:\n",
        "    env = gym.make( game_name)                      \n",
        "env._max_episode_steps = max_steps_for_each_episode\n",
        "\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    summed_reward = 0\n",
        "\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "\n",
        "    # Getting state\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "\n",
        "    for _ in tqdm(range(sys.maxsize)):\n",
        "\n",
        "\n",
        "        # Getting action\n",
        "        state         = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        actions_value = initialize_actions_value(init, noise_t, noise_r,(time_size, action_size) )\n",
        "        actions_value = torch.tensor(actions_value, dtype=torch.float).to(device)\n",
        "        actions_value = update_actions_value(epoch_for_deducing,\n",
        "                                           model_loader,\n",
        "                                           desired_reward,\n",
        "                                           state,\n",
        "                                           actions_value,\n",
        "                                           beta)\n",
        "        action_arg = int(torch.argmax(actions_value[0, 0]))      \n",
        "        action     = vectorizing_action(action_size, action_arg)\n",
        "\n",
        "\n",
        "        # Getting reward\n",
        "        state, reward, done,  info = env.step(action_arg)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "        summed_reward += reward\n",
        "\n",
        "\n",
        "        # Getting state\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged reward:')\n",
        "    summed_reward_sum += summed_reward\n",
        "    print(summed_reward_sum/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
