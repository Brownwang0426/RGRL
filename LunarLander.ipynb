{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AssistMoli/deep_deducing/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0y9RfWif-v2"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For local\n",
        "# cuda==11.8.0 cudnn==8.9.7.29\n",
        "# pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit\n",
        "import gym\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "\n",
        "import multiprocessing\n",
        "import time\n",
        "import csv\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229QV7Hhf-v3",
        "outputId": "83c55d03-f498-4b65-965a-bd08dccb8205"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\") \n",
        "    print('using cpu...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating a class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "# Our multihead-attention code comes from the reference material in following websites. Hoewever, we made some changes to the masking mechanism:\n",
        "# https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
        "# https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
        "# https://ai.plainenglish.io/building-and-training-a-transformer-from-scratch-fdbf3db00df4\n",
        "# However, we are not going to use multihead-attention here. I wrote it down just for future research.\n",
        "\n",
        "# class MultiheadAttention(nn.Module):\n",
        "#     def __init__(self, d_model, num_heads):\n",
        "#         super(MultiheadAttention, self).__init__()\n",
        "#         assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "# \n",
        "#         self.bias = False\n",
        "# \n",
        "#         self.d_model   = d_model\n",
        "#         self.num_heads = num_heads\n",
        "#         self.d_k       = d_model // num_heads\n",
        "# \n",
        "#         self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "#         self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "#         self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "#         self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "# \n",
        "#     def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "# \n",
        "#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "# \n",
        "#         if mask is not None:\n",
        "# \n",
        "#             mask = mask.unsqueeze(1).repeat(1, attn_scores.size(1), 1)\n",
        "#             mask = mask.unsqueeze(3).repeat(1, 1, 1, attn_scores.size(2))\n",
        "#             attn_scores = attn_scores.masked_fill(mask == True, -1e9)\n",
        "#             attn_scores = attn_scores.masked_fill(mask.transpose(-2, -1) == True, -1e9)\n",
        "#         else:\n",
        "#             attn_scores = attn_scores\n",
        "# \n",
        "#         attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "# \n",
        "#         if mask is not None:\n",
        "#             inverted_mask = ~mask\n",
        "#             attn_probs = attn_probs * inverted_mask.type_as(attn_probs)\n",
        "# \n",
        "#         output = torch.matmul(attn_probs, V)\n",
        "# \n",
        "#         return output\n",
        "# \n",
        "#     def split_heads(self, x):\n",
        "#         batch_size, seq_length, d_model = x.size()\n",
        "#         return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "#         #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "# \n",
        "#     def combine_heads(self, x):\n",
        "#         batch_size, _, seq_length, d_k = x.size()\n",
        "#         return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "# \n",
        "#     def forward(self, Q, K, V, mask=None):\n",
        "#         Q = self.split_heads(self.W_q(Q))\n",
        "#         K = self.split_heads(self.W_k(K))\n",
        "#         V = self.split_heads(self.W_v(V))\n",
        "#         attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "#         output      = self.W_o(self.combine_heads(attn_output))\n",
        "#         return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_neuron_size_,\n",
        "                 hidden_neuron_size,\n",
        "                 input_neuron_size,\n",
        "                 input_sequence_size,\n",
        "                 output_neuron_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 initializer,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 alpha,\n",
        "                 mask_value):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.input_neuron_size_   = int(input_neuron_size_)\n",
        "        self.hidden_neuron_size   = int(hidden_neuron_size)\n",
        "        self.input_neuron_size    = int(input_neuron_size)\n",
        "        self.input_sequence_size  = int(input_sequence_size)\n",
        "        self.output_neuron_size   = int(output_neuron_size)\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_heads            = num_heads\n",
        "\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.initializer          = initializer\n",
        "        self.optimizer            = optimizer\n",
        "        self.loss                 = loss\n",
        "        self.alpha                = alpha\n",
        "        self.mask_value           = mask_value\n",
        "\n",
        "        self.bias = False\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        neural_types = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM\n",
        "        }\n",
        "\n",
        "        self.fully_connected_layer_in_0      = nn.Linear(self.input_neuron_size_, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_in_1      = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_out_0     = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_out_1     = nn.Linear(self.hidden_neuron_size, self.input_neuron_size_, bias=self.bias)\n",
        "\n",
        "        self.recurrent_layer_0               = neural_types[neural_type.lower()](self.input_neuron_size, self.hidden_neuron_size, num_layers=self.num_layers, batch_first=False, bias=self.bias)\n",
        "\n",
        "        # self.attention_layer_0               = MultiheadAttention(self.hidden_neuron_size, num_heads= self.num_heads)\n",
        "        # self.att_norm_0                      = nn.LayerNorm(self.hidden_neuron_size)\n",
        "        # self.fully_connected_layer_0         = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "        # self.fc_norm_0                       = nn.LayerNorm(self.hidden_neuron_size)\n",
        "\n",
        "        self.fully_connected_layer_7         = nn.Linear(self.hidden_neuron_size * self.input_sequence_size, self.output_neuron_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.initializer  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.optimizer.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "    def forward(self, initial_hidden, x, padding_mask):\n",
        "\n",
        "        h  = self.fully_connected_layer_in_0(initial_hidden)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_in_1(h)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = torch.unsqueeze(h, dim=0).repeat(self.num_layers, 1, 1)\n",
        "\n",
        "        out          = x.permute(1, 0, 2)\n",
        "        lengths      = (out != self.mask_value).any(dim=2).sum(dim=0).cpu().long() # since x is (sequence_length, batch_size, input_size), we should use sum(dim=0)\n",
        "        out          = rnn_utils.pack_padded_sequence(out, lengths, batch_first=False, enforce_sorted=False)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        if self.neural_type == 'lstm':\n",
        "            out, h     = self.recurrent_layer_0(out, (h, h))\n",
        "            h          = h[0] \n",
        "        else:\n",
        "            out, h     = self.recurrent_layer_0(out, h)\n",
        "            h          = h \n",
        "\n",
        "        out, _     = rnn_utils.pad_packed_sequence(out, batch_first=False)\n",
        "        padding    = (0, 0, 0, 0, 0, self.input_sequence_size - out.size(0))\n",
        "        out        = F.pad(out, padding, \"constant\", 0)\n",
        "        out        = out.permute(1, 0, 2)\n",
        "\n",
        "        # h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_out_0(h)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_out_1(h)\n",
        "        h  = self.output_activation(h)\n",
        "\n",
        "        # if padding_mask is not None:\n",
        "        #     padding_mask = torch.any(padding_mask, dim=-1) # to (batch_size, sequence_length)\n",
        "        # out_ = self.attention_layer_0(out, out, out, padding_mask)\n",
        "        # out  = self.att_norm_0(out + out_)\n",
        "        # out_ = self.fully_connected_layer_0(out)\n",
        "        # out  = self.fc_norm_0(out + out_)\n",
        "\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "\n",
        "        out = self.fully_connected_layer_7(out)\n",
        "        out = self.output_activation(out)\n",
        "\n",
        "        return out, h\n",
        "\n",
        "\n",
        "    def custom_activation(self, x):\n",
        "        return torch.sigmoid(x - 1.5)\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for updating input value using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_action_value(epoch_for_deducing,\n",
        "            model_loader,\n",
        "            desired_reward,\n",
        "            state,\n",
        "            action_value,\n",
        "            beta):\n",
        "\n",
        "    model_loader_ = copy.deepcopy(model_loader)\n",
        "\n",
        "    for epoch in range(epoch_for_deducing):\n",
        "\n",
        "        random.shuffle(model_loader_)\n",
        "\n",
        "        for model in model_loader_:\n",
        "\n",
        "            action = torch.sigmoid(action_value)\n",
        "\n",
        "            action = action.clone().detach().requires_grad_(True)\n",
        "            if action.grad is not None:\n",
        "                action.grad.zero_()\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, _ = model(state, action, padding_mask=None)\n",
        "            total_loss = loss_function(output, desired_reward)\n",
        "\n",
        "            total_loss.backward() # Error Backpropagation\n",
        "            action_value -= action.grad * (1 - action) * action * beta # Update Input Data\n",
        "\n",
        "    return action_value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for updating weight matrices using error backprop\n",
        "\n",
        "Elastic weight consolidation\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkjqdPMozvzh"
      },
      "outputs": [],
      "source": [
        "def update_model(batch_size,\n",
        "                 epoch_for_learning,\n",
        "                 model,\n",
        "                 train_loader,\n",
        "                 dataset,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 prev_train_loader_size,\n",
        "                 EWC_lambda):\n",
        "\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    past_slice = 1000\n",
        "    combined_list = [1] * len(train_loader) + [0] * past_slice\n",
        "    index_list = list(range(len(train_loader) ))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    selected_optimizer = model.selected_optimizer\n",
        "    for param_group in selected_optimizer.param_groups:\n",
        "        param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "    for epoch in range(epoch_for_learning):\n",
        "\n",
        "        random.shuffle(combined_list)\n",
        "        random.shuffle(index_list)\n",
        "        i = 0\n",
        "\n",
        "        for j in range(len(train_loader) + past_slice ):\n",
        "\n",
        "            if combined_list[j] == 1:\n",
        "\n",
        "              state, action, reward, next_state, padding_mask = dataset[index_list[i]]\n",
        "              i+=1\n",
        "\n",
        "              state  = state.unsqueeze(0)\n",
        "              action = action.unsqueeze(0)\n",
        "              reward = reward.unsqueeze(0)\n",
        "              next_state = next_state.unsqueeze(0)\n",
        "              padding_mask = padding_mask.unsqueeze(0)\n",
        "\n",
        "              next_state = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "              selected_optimizer.zero_grad()\n",
        "              loss_function = model.loss_function\n",
        "\n",
        "              output, output_state = model(state, action, padding_mask)\n",
        "              total_loss = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "\n",
        "              total_loss.backward()\n",
        "\n",
        "              selected_optimizer.step() # Update Model Weight\n",
        "\n",
        "            else:\n",
        "\n",
        "              selected_optimizer.zero_grad()\n",
        "\n",
        "              for name, param in model.named_parameters():\n",
        "                    param.grad = (prev_gradient_matrix[name] - (prev_model_param[name] - param) ) * EWC_lambda * ( prev_train_loader_size /past_slice )\n",
        "\n",
        "              selected_optimizer.step() # Update Model Weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    present_gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "\n",
        "        random.shuffle(combined_list)\n",
        "        random.shuffle(index_list)\n",
        "        i = 0\n",
        "\n",
        "        for j in range(len(train_loader)):\n",
        "\n",
        "              state, action, reward, next_state, padding_mask = dataset[index_list[i]]\n",
        "              i+=1\n",
        "\n",
        "              state  = state.unsqueeze(0)\n",
        "              action = action.unsqueeze(0)\n",
        "              reward = reward.unsqueeze(0)\n",
        "              next_state = next_state.unsqueeze(0)\n",
        "              padding_mask = padding_mask.unsqueeze(0)\n",
        "\n",
        "              next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "              selected_optimizer.zero_grad()\n",
        "              loss_function = model.loss_function\n",
        "\n",
        "              output, output_state = model(state, action, padding_mask)\n",
        "              total_loss = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "\n",
        "              total_loss.backward()\n",
        "\n",
        "              for name, param in model.named_parameters():\n",
        "                    present_gradient_matrix[name] += param.grad\n",
        "\n",
        "        for j in range(1):\n",
        "\n",
        "              selected_optimizer.zero_grad()\n",
        "\n",
        "              for name, param in model.named_parameters():\n",
        "                    param.grad = (prev_gradient_matrix[name] - (prev_model_param[name] - param) ) * EWC_lambda * prev_train_loader_size\n",
        "\n",
        "\n",
        "    present_gradient_matrix = {name: param / ( (len(train_loader) + prev_train_loader_size) ) for name, param in present_gradient_matrix.items()}\n",
        "\n",
        "    present_train_loader_size = len(train_loader) + prev_train_loader_size\n",
        "\n",
        "    return model, present_gradient_matrix, present_train_loader_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Directional EWC\n",
        "def EWC_loss( EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size):\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        loss += ( ((present_gradient_matrix[name] - prev_gradient_matrix[name])     *     (param - prev_model_param[name])) **2 ).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "# Gradient-based EWC\n",
        "def EWC_loss( EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size):\n",
        "    present_model_param = present_model.state_dict()\n",
        "    prev_model_param    = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        loss += (  (( (present_gradient_matrix[name] - (present_model_param[name]  - param)) -\n",
        "                      (prev_gradient_matrix[name]    - (prev_model_param[name]     - param))   )**2   )\n",
        "                  *((  param - prev_model_param[name]   )**2                                                )\n",
        "                  *((  param - present_model_param[name])**2                                                )        ).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "# Traditional EWC\n",
        "def EWC_loss( EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size):\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        loss += ( ((prev_gradient_matrix[name])**2) * ((param - prev_model_param[name])**2)     ).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "def update_model(batch_size,\n",
        "                 epoch_for_learning,\n",
        "                 model,\n",
        "                 train_loader,\n",
        "                 dataset,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 prev_train_loader_size,\n",
        "                 EWC_lambda):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # retrieving present_gradient_matrix\n",
        "    present_model = copy.deepcopy(model)\n",
        "    present_gradient_matrix =  {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward) + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            for name, param in model.named_parameters():\n",
        "                present_gradient_matrix[name] += param.grad\n",
        "\n",
        "    present_gradient_matrix = {name: param / len(train_loader) for name, param in present_gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epoch_for_learning):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward)  + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss += EWC_loss(EWC_lambda, model, present_model, present_gradient_matrix, prev_model, prev_gradient_matrix, prev_train_loader_size)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            selected_optimizer.step() # Update Model Weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # training and updating present_gradient_matrix\n",
        "    updated_present_gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        for param_group in selected_optimizer.param_groups:\n",
        "            param_group['lr'] = model.alpha * batch_size\n",
        "\n",
        "        for state, action, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            next_state  = torch.unsqueeze(next_state, dim=0).repeat(model.num_layers, 1, 1)\n",
        "\n",
        "            selected_optimizer.zero_grad()\n",
        "            loss_function = model.loss_function\n",
        "\n",
        "            output, output_state = model(state, action, padding_mask)\n",
        "            total_loss = loss_function(output, reward)  + loss_function(output_state, next_state)\n",
        "\n",
        "            total_loss.backward()     # Error Backpropagation\n",
        "\n",
        "            for name, param in model.named_parameters():\n",
        "                updated_present_gradient_matrix[name] += param.grad \n",
        "\n",
        "    updated_present_gradient_matrix = {name: param / len(train_loader) for name, param in updated_present_gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    present_train_loader_size = len(train_loader) + prev_train_loader_size\n",
        "\n",
        "    return model, present_gradient_matrix, present_train_loader_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for re-initialize action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_input(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for sequentialize state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qGocnYQJ4FO"
      },
      "outputs": [],
      "source": [
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "\n",
        "    sequentialized_state_list  = []\n",
        "    sequentialized_action_list = []\n",
        "    sequentialized_reward_list = []\n",
        "    sequentialized_next_state_list  = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    time_size_ = time_size \n",
        "    for i in range(len(action_list)):\n",
        "        sequentialized_state_list.append( state_list[i ] )\n",
        "        sequentialized_action_list.append( action_list[i:i+time_size_]  )\n",
        "        sequentialized_reward_list.append( reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "        sequentialized_next_state_list.append(  state_list[ i + len(action_list[i:i+time_size_]) ]  )\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pAeDkANCOE0"
      },
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for converting state into concatenated one-hot vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def retreive_state(state, main_engine_fire_t, side_engine_fire_t):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0 = quantifying(100, -1.5, 0.03 * 1, state[0]) \n",
        "    state_1 = quantifying(100, -1.5, 0.03 * 1, state[1]) \n",
        "    state_2 = quantifying(100, -1.5, 0.03 * 1, state[2]) \n",
        "    state_3 = quantifying(100, -1.5, 0.03 * 1, state[3]) \n",
        "    state_4 = quantifying(100, -1, 0.02 * 1, state[4])   \n",
        "    state_5 = quantifying(100, -1, 0.02 * 1, state[5])   \n",
        "    state_6 = quantifying(100, 0, 0.01 * 1, state[6])    \n",
        "    state_7 = quantifying(100, 0, 0.01 * 1, state[7])    \n",
        "    state_8 = quantifying(100, 0, 1, main_engine_fire_t) \n",
        "    state_9 = quantifying(100, 0, 1, side_engine_fire_t) \n",
        "    # state_10 = np.mean(np.array(env.render(mode='rgb_array')), axis=2, keepdims=True).flatten() / 255  \n",
        "    state_all    = np.atleast_2d(np.concatenate((state_0, state_1, state_2, state_3, state_4, state_5, state_6, state_7, state_8, state_9)))   \n",
        "    return state_all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Note of experience:\n",
        "\n",
        "In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n",
        "Also, it's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"             # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "max_steps_for_each_episode = 200         # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "state_size = 1000                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "hidden_size = 250                        # Reminder: change this for your specific task ⚠️⚠️⚠️ (should be dividable by num_heads below)\n",
        "action_size = 4                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "time_size = 25                           # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "chunk_size = 25                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "reward_size = 250                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "ensemble_size = 10                       # Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️\n",
        "neural_type = 'gru'                      # rnn gru lstm\n",
        "num_layers = 2                           # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "num_heads  = 10                          # should be able to divide hidden_size\n",
        "hidden_activation = 'tanh'               # relu leaky_relu sigmoid tanh\n",
        "output_activation = 'sigmoid'            # relu leaky_relu sigmoid tanh\n",
        "init = \"random_normal\"                   # random_normal random_uniform xavier_normal xavier_uniform  glorot_normal  glorot_uniform\n",
        "loss = 'mean_squared_error'              # mean_squared_error  binary_crossentropy\n",
        "opti = 'sgd'                             # adam sgd rmsprop\n",
        "alpha = 0.1\n",
        "epoch_for_learning = 10\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "noise_t = 1\n",
        "noise_r = 0.1\n",
        "beta = 0.1\n",
        "epoch_for_deducing =  int(100/ensemble_size)\n",
        "\n",
        "\n",
        "episode_for_training             = 100000\n",
        "replay_range                     = 2                    # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "interval_for_initiating_learning = 50                   # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "EWC_lambda = 1                                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "\n",
        "episode_for_testing = 100                # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "render_for_human = False                 # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mask_value = sys.maxsize\n",
        "load_pre_model = False\n",
        "suffix                      = f\"ensemble={ensemble_size:05d}_learn={epoch_for_learning:05d}_interval={interval_for_initiating_learning:05d}_deduce={epoch_for_deducing:05d}\"\n",
        "directory                   = f'/content/deep_deducing/{game_name}/'\n",
        "model_directory             = f'/content/deep_deducing/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/deep_deducing/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning > Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating intial gradient matrices "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "storing previous models\n",
        "\"\"\"\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "prev_train_loader_size = 1\n",
        "\n",
        "\"\"\"\n",
        "calculating gradient matrix\n",
        "\"\"\"\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "82616cd1-01db-49b6-a643-4a7ce74ad7cd"
      },
      "outputs": [],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "episode_list = []\n",
        "sequentialized_state_list = []\n",
        "sequentialized_action_list = []\n",
        "sequentialized_reward_list = []\n",
        "sequentialized_next_state_list = []\n",
        "\n",
        "env = gym.make(game_name)  \n",
        "env._max_episode_steps = max_steps_for_each_episode\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    main_engine_fire_t = 0 \n",
        "    side_engine_fire_t = 0 \n",
        "    state_all = retreive_state(state, main_engine_fire_t, side_engine_fire_t)\n",
        "    state_list.append(state_all[0])\n",
        "    state_all = torch.tensor(state_all, dtype=torch.float).to(device)\n",
        "\n",
        "    for _ in range(sys.maxsize):\n",
        "\n",
        "        action_value = initialize_input(init, noise_t, noise_r,(time_size, action_size) )\n",
        "        action_value = torch.tensor(action_value, dtype=torch.float).to(device)\n",
        "        action_value = update_action_value(epoch_for_deducing,\n",
        "                          model_loader,\n",
        "                          desired_reward,\n",
        "                          state_all,\n",
        "                          action_value,\n",
        "                          beta)\n",
        "\n",
        "        action = int(torch.argmax(action_value[0, 0]))\n",
        "        state, reward, done,  info = env.step(action)\n",
        "        summed_reward += reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        reward = quantifying(reward_size, -400, (350 - (-400))/reward_size, reward)       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        action_list.append(np.eye(action_size)[action])\n",
        "        reward_list.append(reward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if action == 2:                                     \n",
        "            main_engine_fire_t += 1                         \n",
        "        elif (action == 1) or (action == 3):                \n",
        "            side_engine_fire_t += 1                         \n",
        "        state_all = retreive_state(state, main_engine_fire_t, side_engine_fire_t)\n",
        "        state_list.append(state_all[0])\n",
        "        state_all = torch.tensor(state_all, dtype=torch.float).to(device)\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode+1, summed_reward])\n",
        "            # Save performance log to CSV\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    sequentializing and setting replay range\n",
        "    \"\"\"\n",
        "    sequentialized_state_list_slice, sequentialized_action_list_slice, sequentialized_reward_list_slice, sequentialized_next_state_list_slice = sequentialize(state_list, action_list, reward_list, chunk_size )\n",
        "\n",
        "    episode_list .extend( [ training_episode ] * len(sequentialized_state_list_slice))\n",
        "    sequentialized_state_list       .extend( sequentialized_state_list_slice)\n",
        "    sequentialized_action_list      .extend( sequentialized_action_list_slice)\n",
        "    sequentialized_reward_list      .extend( sequentialized_reward_list_slice)\n",
        "    sequentialized_next_state_list  .extend( sequentialized_next_state_list_slice)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if (training_episode+1) % interval_for_initiating_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        episode_list                   = [i for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_state_list      = [a for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_action_list     = [b for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_reward_list     = [c for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "        sequentialized_next_state_list = [d for i, a, b, c, d in zip(episode_list, sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list) if ((training_episode - replay_range * interval_for_initiating_learning + 1) <= i <= training_episode)     ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        masking and uploading data\n",
        "        \"\"\"\n",
        "\n",
        "        state_tensor      = torch.stack( [torch.tensor(arr) for arr in sequentialized_state_list]               ).float().to(device)\n",
        "        action_tensor     = torch.stack( [F.pad(torch.tensor(arr), \n",
        "                                                pad=(0, 0, 0, time_size - torch.tensor(arr).size(0)), \n",
        "                                                mode='constant', \n",
        "                                                value= mask_value) for arr in sequentialized_action_list]       ).float().to(device)\n",
        "        reward_tensor     = torch.stack( [torch.tensor(arr) for arr in sequentialized_reward_list]              ).float().to(device)\n",
        "        next_state_tensor = torch.stack( [torch.tensor(arr) for arr in sequentialized_next_state_list]          ).float().to(device)\n",
        "\n",
        "        row_mask     = torch.all(action_tensor == mask_value, dim = -1)\n",
        "        padding_mask = torch.zeros_like(action_tensor, dtype = torch.bool)\n",
        "        padding_mask[row_mask] = True\n",
        "        padding_mask = padding_mask.to(device)\n",
        "\n",
        "        dataset     = TensorDataset(state_tensor, action_tensor, reward_tensor, next_state_tensor, padding_mask)\n",
        "        data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        learning\n",
        "        \"\"\"\n",
        "        gradient_matrix_loader_ = []\n",
        "        for i, model in enumerate(model_loader):\n",
        "            model, gradient_matrix_, train_loader_size_= update_model(batch_size, epoch_for_learning, model, data_loader, dataset, prev_model_loader[i], prev_gradient_matrix_loader[i], prev_train_loader_size, EWC_lambda)\n",
        "            gradient_matrix_loader_.append(gradient_matrix_)\n",
        "            model_loader[i] = model\n",
        "        prev_gradient_matrix_loader = gradient_matrix_loader_\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_train_loader_size      = train_loader_size_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        saving\n",
        "        \"\"\"\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,  \n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        alpha,\n",
        "                        mask_value)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "putting all the previous works into play ... again\n",
        "\n",
        "but this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "summed_reward_sum = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    summed_reward = 0\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")   \n",
        "    else:\n",
        "        env = gym.make( game_name)                    \n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "\n",
        "    main_engine_fire_t = 0 \n",
        "    side_engine_fire_t = 0 \n",
        "    state_all = retreive_state(state, main_engine_fire_t, side_engine_fire_t)\n",
        "    state_all = torch.tensor(state_all, dtype=torch.float).to(device)\n",
        "\n",
        "    for _ in tqdm(range(max_steps_for_each_episode)):\n",
        "\n",
        "        action_value = initialize_input(init, noise_t, noise_r, (time_size, action_size) )\n",
        "        action_value = torch.tensor(action_value, dtype=torch.float).to(device)\n",
        "        action_value = update_action_value(epoch_for_deducing,\n",
        "                          model_loader,\n",
        "                          desired_reward,\n",
        "                          state_all,\n",
        "                          action_value,\n",
        "                          beta)\n",
        "\n",
        "        action = int(torch.argmax(action_value[0, 0]))\n",
        "        state, reward, done,  info = env.step(action)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "        summed_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if action == 2:                                  \n",
        "            main_engine_fire_t += 1                      \n",
        "        elif (action == 1) or (action == 3):             \n",
        "            side_engine_fire_t += 1                      \n",
        "        state_all = retreive_state(state, main_engine_fire_t, side_engine_fire_t)\n",
        "        state_all = torch.tensor(state_all, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged reward:')\n",
        "    summed_reward_sum += summed_reward\n",
        "    print(summed_reward_sum/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
