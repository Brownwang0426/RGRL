{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/CartPole_nested_recurrent_attention_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "3350c060-7302-4eeb-bfc9-f73f03974e0e"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0y9RfWif-v2",
        "outputId": "31b7bb14-f74e-4aaf-fe69-4cebbdb8bbcf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "af231b3d-b797-492f-b1ad-2d5ce1b94ff0"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYykFEEVSxd9"
      },
      "source": [
        "# Class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "class custom_attn(nn.Module):\n",
        "    def __init__(self, d_model, num_heads = 8):\n",
        "        super(custom_attn, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.bias      = False\n",
        "        self.d_model   = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k       = d_model // num_heads\n",
        "\n",
        "        self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "        if mask != None:\n",
        "            attn_scores += mask\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q    -> (batch_size, seq_length, d_model)\n",
        "        # mask -> (batch_size, 1, seq_length, d_model)\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output      = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 reward_size,\n",
        "                 hidden_size,\n",
        "                 time_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 init,\n",
        "                 opti,\n",
        "                 loss,\n",
        "                 drop_rate,\n",
        "                 alpha):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.state_size           = state_size\n",
        "        self.action_size          = action_size\n",
        "        self.reward_size          = reward_size\n",
        "        self.hidden_size          = hidden_size\n",
        "        self.time_size            = time_size\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_layers           = num_layers\n",
        "        self.num_heads            = num_heads\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.init                 = init\n",
        "        self.opti                 = opti\n",
        "        self.loss                 = loss\n",
        "        self.drop_rate            = drop_rate\n",
        "        self.alpha                = alpha\n",
        "\n",
        "        self.bias                 = False\n",
        "\n",
        "        self.state_linear         = nn.Linear(self.state_size, self.hidden_size, bias=self.bias)\n",
        "        self.action_linear        = nn.Linear(self.action_size , self.hidden_size, bias=self.bias)\n",
        "        self.positional_encoding  = nn.Parameter(self.generate_positional_encoding(2, self.hidden_size ), requires_grad=False)\n",
        "        self.transformer_layers   = \\\n",
        "        nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                custom_attn(self.hidden_size, self.num_heads),\n",
        "                nn.LayerNorm(self.hidden_size),\n",
        "                nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias),\n",
        "                nn.LayerNorm(self.hidden_size)\n",
        "            ])\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "        self.reward_linear        = nn.Linear(self.hidden_size, self.reward_size, bias=self.bias)\n",
        "        self.state_linear_        = nn.Linear(self.hidden_size, self.state_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.init  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.opti.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(reduction='none'),\n",
        "            'binary_crossentropy': torch.nn.BCELoss(reduction='none')\n",
        "        }\n",
        "        self.loss_function_ = losses[self.loss .lower()]\n",
        "\n",
        "\n",
        "    def forward(self, s, a_list):\n",
        "        \n",
        "        mask = None\n",
        "\n",
        "        r_list = list()\n",
        "        s_list = list()\n",
        "\n",
        "        s  = self.state_linear(s)\n",
        "        s  = self.hidden_activation(s)\n",
        "\n",
        "        a_ = self.action_linear(a_list[:,0])\n",
        "        a_ = self.hidden_activation(a_)\n",
        "\n",
        "        h  = torch.stack([s, a_], dim=0).view(a_.size(0), 2, a_.size(1))\n",
        "        h  = h + self.positional_encoding[:, :, :]\n",
        "\n",
        "        pres_h_list = list()\n",
        "        for j, layer in enumerate(self.transformer_layers):\n",
        "            attention_layer, attention_norm_layer, fully_connected_layer, fully_connected_norm_layer = layer\n",
        "            h_ = attention_layer(h, h, h, mask)\n",
        "            h  = attention_norm_layer(h + h_)\n",
        "            h_ = fully_connected_layer(h)\n",
        "            h  = fully_connected_norm_layer(h + h_)\n",
        "            pres_h_list.append(h)\n",
        "        prev_h_list = pres_h_list\n",
        "\n",
        "        r  = h[:, 0]\n",
        "        s_ = h[:, 1]\n",
        "        \n",
        "        r  = self.reward_linear(r)   \n",
        "        r  = self.custom_activation(r)\n",
        "\n",
        "        s_ = self.state_linear_(s_)   \n",
        "        s_ = self.output_activation(s_)\n",
        "\n",
        "        s  = self.state_linear(s_)\n",
        "        s  = self.hidden_activation(s)\n",
        "\n",
        "        r_list.append(r)\n",
        "        s_list.append(s_)\n",
        "\n",
        "        for i in range(a_list.size(1)-1):\n",
        "\n",
        "            a_ = self.action_linear(a_list[:,i+1])\n",
        "            a_ = self.hidden_activation(a_)\n",
        "\n",
        "            h  = torch.stack([s, a_], dim=0).view(a_.size(0), 2, a_.size(1))\n",
        "            h  = h + self.positional_encoding[:, :, :]\n",
        "\n",
        "            pres_h_list = list()\n",
        "            for j, layer in enumerate(self.transformer_layers):\n",
        "                attention_layer, attention_norm_layer, fully_connected_layer, fully_connected_norm_layer = layer\n",
        "                h_ = attention_layer(prev_h_list[j], prev_h_list[j], h, mask)\n",
        "                h  = attention_norm_layer(h + h_)\n",
        "                h_ = fully_connected_layer(h)\n",
        "                h  = fully_connected_norm_layer(h + h_)\n",
        "                pres_h_list.append(h)\n",
        "            prev_h_list = pres_h_list\n",
        "\n",
        "            r  = h[:, 0]\n",
        "            s_ = h[:, 1]\n",
        "            \n",
        "            r  = self.reward_linear(r)   \n",
        "            r  = self.custom_activation(r)\n",
        "\n",
        "            s_ = self.state_linear_(s_)   \n",
        "            s_ = self.output_activation(s_)\n",
        "\n",
        "            s  = self.state_linear(s_)\n",
        "            s  = self.hidden_activation(s)\n",
        "\n",
        "            r_list.append(r)\n",
        "            s_list.append(s_)\n",
        "\n",
        "        r_list = torch.stack(r_list, dim=1)\n",
        "        s_list = torch.stack(s_list, dim=1)\n",
        "        \n",
        "        return r_list, s_list\n",
        "\n",
        "\n",
        "    def generate_positional_encoding(self, max_len, model_dim):\n",
        "        pe = torch.zeros(max_len,model_dim)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0,model_dim,2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/model_dim)))\n",
        "                if i + 1 < model_dim:\n",
        "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/model_dim)))\n",
        "        return pe.unsqueeze(0)  # Shape: (1, max_len, model_dim)\n",
        "\n",
        "    def custom_activation(self, x):\n",
        "        return torch.sigmoid(x + 1.5)\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYsyx03Sxd_"
      },
      "source": [
        "# Function for updating pre-activated action using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_pre_activated_future_action(iteration_for_deducing,\n",
        "                                       model_loader,\n",
        "                                       state,\n",
        "                                       pre_activated_future_action,\n",
        "                                       desired_future_reward,\n",
        "                                       beta):\n",
        "\n",
        "    model_loader_copy = copy.deepcopy(model_loader)\n",
        "\n",
        "    for _ in range(iteration_for_deducing):\n",
        "\n",
        "        model   = random.choice(model_loader_copy)\n",
        "\n",
        "        future_action = torch.sigmoid(pre_activated_future_action)\n",
        "\n",
        "        model.train()\n",
        "        future_action = future_action.clone().detach().requires_grad_(True)\n",
        "        if future_action.grad is not None:\n",
        "            future_action.grad.zero_()\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        loss_function       = model.loss_function\n",
        "        output_reward, _    = model(state, future_action)\n",
        "        total_loss          = loss_function(output_reward, desired_future_reward)\n",
        "        total_loss.backward() # get grad\n",
        "\n",
        "        pre_activated_future_action -= future_action.grad * (1 - future_action) * future_action * beta # update params\n",
        "\n",
        "    return pre_activated_future_action\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yskW6bb1SxeA"
      },
      "source": [
        "# Function for updating model using error backprop\n",
        "\n",
        "Elastic weight consolidation:\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rLQCQigdSxeA"
      },
      "outputs": [],
      "source": [
        "# traditional EWC\n",
        "def EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix):\n",
        "    model_param      = model.state_dict()\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        diagonal_fisher_matrix = prev_gradient_matrix[name] ** 2\n",
        "        param_diff             = (model_param[name] - prev_model_param[name]) ** 2\n",
        "        loss                  += (diagonal_fisher_matrix * param_diff).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_model(iteration_for_learning,\n",
        "                 model,\n",
        "                 sub_data_loader,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 EWC_lambda):\n",
        "    \n",
        "    for state, future_action, future_reward, future_state in sub_data_loader:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function               = model.loss_function\n",
        "        output_reward, output_state = model(state, future_action)\n",
        "        total_loss                  = loss_function(output_reward, future_reward) + loss_function(output_state, future_state)\n",
        "        total_loss                 += EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix)\n",
        "        total_loss.backward()     # get grad\n",
        "\n",
        "        selected_optimizer.step() # update params\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_gradient_matrix(model,\n",
        "                           data_loader):\n",
        "    \n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for state, future_action, future_reward, future_state in data_loader:\n",
        "            \n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function               = model.loss_function\n",
        "        output_reward, output_state = model(state, future_action)\n",
        "        total_loss                  = loss_function(output_reward, future_reward) + loss_function(output_state, future_state)\n",
        "        total_loss.backward()        # get grad\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if name != \"positional_encoding\":\n",
        "                gradient_matrix[name] += param.grad\n",
        "\n",
        "    gradient_matrix = {name: param / len(data_loader) for name, param in gradient_matrix.items()}\n",
        "\n",
        "    return gradient_matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrw4zh7SxeB"
      },
      "source": [
        "# Function for re-initializing action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_pre_activated_future_action(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for vectorizing\n",
        "Crucial function regarding how you manipulate or shape your state, action and reward\n",
        "\n",
        "- It's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "- As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def vectorizing_state(state):      # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0 = quantifying(100, -2.5, 0.050, state[0])\n",
        "    state_1 = quantifying(100, -3.75, 0.075, state[1])\n",
        "    state_2 = quantifying(100, -0.375, 0.0075, state[2])\n",
        "    state_3 = quantifying(100, -3.75, 0.075, state[3])\n",
        "    state_4 = quantifying(100, 0, 10, 0)\n",
        "    state   = np.atleast_2d(np.concatenate((state_0, state_1, state_2, state_3, state_4)))\n",
        "    return state\n",
        "\n",
        "def vectorizing_action(action_size, action_argmax):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    return np.eye(action_size)[action_argmax]\n",
        "\n",
        "def vectorizing_reward(state, reward, summed_reward, reward_size):       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    if reward != 1:\n",
        "        reward = np.zeros(reward_size)\n",
        "    else:\n",
        "        reward = np.ones(reward_size)\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENsHGddSxeC"
      },
      "source": [
        "# Function for sequentializing state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size):\n",
        "\n",
        "    short_term_present_state_list = []\n",
        "    short_term_future_action_list = []\n",
        "    short_term_future_reward_list = []\n",
        "    short_term_future_state_list  = []\n",
        "\n",
        "    if time_size > len(short_term_state_list[:-1]):\n",
        "        time_size = len(short_term_state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    for i in range(len(short_term_reward_list[:-time_size+1])):\n",
        "        short_term_present_state_list.append(      short_term_state_list [ i                       ]  )\n",
        "        short_term_future_action_list.append(      short_term_action_list[ i   : i+time_size       ]  )\n",
        "        short_term_future_reward_list.append(      short_term_reward_list[ i   : i+time_size       ]  )\n",
        "        short_term_future_state_list.append(       short_term_state_list [ i+1 : i+time_size+1     ]  )\n",
        "\n",
        "    return short_term_present_state_list, short_term_future_action_list, short_term_future_reward_list, short_term_future_state_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTosZ2u01xa"
      },
      "source": [
        "# Function for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1zJs9Dgv01xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_from_list(short_term_state_list,\n",
        "                            short_term_future_action_list,\n",
        "                            short_term_future_reward_list,\n",
        "                            short_term_future_state_list,\n",
        "                            device):\n",
        "\n",
        "    # Convert lists to tensors directly on the desired device and data type\n",
        "    short_term_state_tensor         = torch.tensor(np.array(short_term_state_list), dtype=torch.float).to(device)\n",
        "    short_term_future_action_tensor = torch.tensor(np.array(short_term_future_action_list), dtype=torch.float).to(device)\n",
        "    short_term_future_reward_tensor = torch.tensor(np.array(short_term_future_reward_list), dtype=torch.float).to(device)\n",
        "    short_term_future_state_tensor  = torch.tensor(np.array(short_term_future_state_list), dtype=torch.float).to(device)\n",
        "\n",
        "    return short_term_state_tensor, short_term_future_action_tensor, short_term_future_reward_tensor, short_term_future_state_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def obtain_TD_error(model,\n",
        "                    train_loader_,\n",
        "                    device):\n",
        "\n",
        "\n",
        "    for state, future_action, future_reward, future_state in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function_\n",
        "        output_reward, _     = model(state, future_action)\n",
        "        total_loss           = loss_function(output_reward, future_reward).detach().to(device)\n",
        "        total_loss           = torch.sum(torch.abs(total_loss), dim=(1, 2))\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'                # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "max_steps_for_each_episode = 2000        # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "ensemble_size = 10                # choose the size of the neural ensemble (Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️)\n",
        "state_size =  500                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 2                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 100                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 20                    # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'att'               # choose your neural type: [rnn gru lstm] [att]\n",
        "num_layers = 2                    # choose the number of layers for your rnn or attention: [1 2 3 4 etc.]\n",
        "num_heads = 10                    # choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu leaky_relu sigmoid tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu leaky_relu sigmoid tanh]\n",
        "init = \"random_normal\"            # choose initialization method: [random_normal random_uniform xavier_normal xavier_uniform glorot_normal glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam sgd rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error binary_crossentropy]\n",
        "drop_rate = 0.0001                # choose your drop rate (Reminder: change this value to see the impact of drop-out ◀️◀️◀️)\n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "iteration_for_learning = 10000    # choose learning iteration for nn weights (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1                      # gaussian noise\n",
        "noise_r = 0.1                    # smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "beta = 0.1                       # updating rate for input action\n",
        "iteration_for_deducing =  100    # updating iteration for input action (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 10     # batch size for batch offline learning (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "PER_exponent = 1                         # prioritized_experience_replay (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "PER_replace = True                       # prioritized_experience_replay sample method (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "EWC_lambda = 1                           # elastic weight control lambda (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            action_size,\n",
        "                            reward_size,\n",
        "                            hidden_size,\n",
        "                            time_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            action_size,\n",
        "                            reward_size,\n",
        "                            hidden_size,\n",
        "                            time_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n",
        "\n",
        "gradient_matrix_loader = [''] * len(model_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulHSk5_SxeG"
      },
      "source": [
        "Creating intial gradient matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_future_reward = torch.ones((batch_size, time_size, reward_size)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "df8f8daf-8e05-41fb-dd81-5dc6983bd6ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env           = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state         = env.reset()\n",
        "    summed_step   = 0\n",
        "    summed_reward = 0\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state)\n",
        "    short_term_state_list.append(state[0])\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        \n",
        "\n",
        "        # initializing and updating action\n",
        "        state                       = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = initialize_pre_activated_future_action(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_future_action = torch.tensor(pre_activated_future_action, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = update_pre_activated_future_action(iteration_for_deducing,\n",
        "                                                                  model_loader,\n",
        "                                                                  state,\n",
        "                                                                  pre_activated_future_action,\n",
        "                                                                  desired_future_reward,\n",
        "                                                                  beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_future_action[0, 0]))\n",
        "        action           = vectorizing_action(action_size, action_argmax)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_argmax)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_step += 1\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state)\n",
        "        short_term_state_list.append(state[0])\n",
        "\n",
        "\n",
        "        if summed_step < time_size:\n",
        "            done = False\n",
        "        else:\n",
        "            if done:\n",
        "                print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "                performance_log.append([training_episode+1, summed_reward])\n",
        "                save_performance_to_csv(performance_log, performance_log_directory)\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer \n",
        "    short_term_state_list, \\\n",
        "    short_term_future_action_list, \\\n",
        "    short_term_future_reward_list, \\\n",
        "    short_term_future_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size )\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # saving short term experience replay buffer to long term experience replay buffer\n",
        "    short_term_state_tensor,\\\n",
        "    short_term_future_action_tensor,\\\n",
        "    short_term_future_reward_tensor,\\\n",
        "    short_term_future_state_tensor = obtain_tensor_from_list(short_term_state_list,\n",
        "                                                             short_term_future_action_list,\n",
        "                                                             short_term_future_reward_list,\n",
        "                                                             short_term_future_state_list,\n",
        "                                                             device)\n",
        "    if training_episode==0:\n",
        "        long_term_state_tensor               = copy.deepcopy(short_term_state_tensor)\n",
        "        long_term_future_action_tensor       = copy.deepcopy(short_term_future_action_tensor)\n",
        "        long_term_future_reward_tensor       = copy.deepcopy(short_term_future_reward_tensor)\n",
        "        long_term_future_state_tensor        = copy.deepcopy(short_term_future_state_tensor)\n",
        "    else:\n",
        "        long_term_state_tensor               = torch.cat((long_term_state_tensor              , short_term_state_tensor            ), dim=0)\n",
        "        long_term_future_action_tensor       = torch.cat((long_term_future_action_tensor      , short_term_future_action_tensor    ), dim=0)\n",
        "        long_term_future_reward_tensor       = torch.cat((long_term_future_reward_tensor      , short_term_future_reward_tensor    ), dim=0)\n",
        "        long_term_future_state_tensor        = torch.cat((long_term_future_state_tensor       , short_term_future_state_tensor     ), dim=0)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # creating dataset\n",
        "        dataset      = TensorDataset(long_term_state_tensor          ,\n",
        "                                     long_term_future_action_tensor  ,\n",
        "                                     long_term_future_reward_tensor  ,\n",
        "                                     long_term_future_state_tensor   )\n",
        "        data_loader  = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER) and Elastic Weight Control (EWC)\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # creating TD error probability\n",
        "                TD_error     = obtain_TD_error(model, data_loader, device)\n",
        "                TD_error     = TD_error.cpu().numpy() ** PER_exponent\n",
        "                TD_error_p   = TD_error / np.sum(TD_error)\n",
        "\n",
        "                # creating sub dataset and sub data loader from  TD error probability\n",
        "                if PER_replace == True:\n",
        "                    index_list       = np.random.choice(range(len(dataset)), \n",
        "                                                        p = TD_error_p, \n",
        "                                                        size = iteration_for_learning, \n",
        "                                                        replace=PER_replace)\n",
        "                else:\n",
        "                    index_list       = np.random.choice(range(len(dataset)), \n",
        "                                                        p = TD_error_p, \n",
        "                                                        size = min(iteration_for_learning, len(dataset)),\n",
        "                                                        replace=PER_replace)\n",
        "                sub_dataset      = Subset(dataset, index_list)\n",
        "                sub_data_loader  = DataLoader(sub_dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # training with PER and EWC\n",
        "                model                     = update_model(iteration_for_learning,\n",
        "                                                         model,\n",
        "                                                         sub_data_loader,\n",
        "                                                         prev_model_loader[i],\n",
        "                                                         prev_gradient_matrix_loader[i],\n",
        "                                                         EWC_lambda)\n",
        "                model_loader[i]           = model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # obtaining EWC gradient\n",
        "                gradient_matrix           = update_gradient_matrix(model,\n",
        "                                                                   data_loader)\n",
        "                gradient_matrix_loader[i] = gradient_matrix\n",
        "        torch.cuda.synchronize()\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_gradient_matrix_loader = copy.deepcopy(gradient_matrix_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        hidden_size,\n",
        "                        time_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_future_reward = torch.ones((batch_size, time_size, reward_size)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                       = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = initialize_pre_activated_future_action(init, noise_t, noise_r, (1, time_size, action_size))\n",
        "        pre_activated_future_action = torch.tensor(pre_activated_future_action, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = update_pre_activated_future_action(iteration_for_deducing,\n",
        "                                                                  model_loader,\n",
        "                                                                  state,\n",
        "                                                                  pre_activated_future_action,\n",
        "                                                                  desired_future_reward,\n",
        "                                                                  beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_future_action[0, 0]))\n",
        "\n",
        "        state, reward, done,  info = env.step(action_argmax)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
