{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/model_in_experiment/LunarLander_nested_recurrent_attention_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "1aa9c6af-7c36-4903-c290-d1a2d3f143ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m0y9RfWif-v2",
        "outputId": "e18c3d47-3cfc-4d93-a110-ac552fdb67f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.2.1\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting ufal.pybox2d==2.3.10.3\n",
            "  Downloading ufal.pybox2d-2.3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (416 bytes)\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting pygame==2.5.2\n",
            "  Downloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.3)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ufal.pybox2d-2.3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ufal.pybox2d, swig, lit, pygame, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, scipy, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.0\n",
            "    Uninstalling pygame-2.6.0:\n",
            "      Successfully uninstalled pygame-2.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.0.1 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 numpy-1.25.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.3 pygame-2.5.2 scipy-1.11.4 swig-4.2.1 torch-2.0.1 triton-2.0.0 ufal.pybox2d-2.3.10.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "090cf7eae7fc4048a6a59851d765377e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "9c3b4888-3468-401f-82d6-ff763e4396e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0: Tesla T4\n",
            "using cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYykFEEVSxd9"
      },
      "source": [
        "# Class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "class custom_attn(nn.Module):\n",
        "    def __init__(self, d_model, num_heads = 8):\n",
        "        super(custom_attn, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.bias      = False\n",
        "        self.d_model   = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k       = d_model // num_heads\n",
        "\n",
        "        self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "        self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "        if mask != None:\n",
        "            attn_scores += mask\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q    -> (batch_size, seq_length, d_model)\n",
        "        # mask -> (batch_size, 1, seq_length, d_model)\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output      = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 reward_size,\n",
        "                 hidden_size,\n",
        "                 time_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 init,\n",
        "                 opti,\n",
        "                 loss,\n",
        "                 drop_rate,\n",
        "                 alpha):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.state_size           = state_size\n",
        "        self.action_size          = action_size\n",
        "        self.reward_size          = reward_size\n",
        "        self.hidden_size          = hidden_size\n",
        "        self.time_size            = time_size\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_layers           = num_layers\n",
        "        self.num_heads            = num_heads\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.init                 = init\n",
        "        self.opti                 = opti\n",
        "        self.loss                 = loss\n",
        "        self.drop_rate            = drop_rate\n",
        "        self.alpha                = alpha\n",
        "\n",
        "        self.bias                 = False\n",
        "\n",
        "        self.state_linear         = nn.Linear(self.state_size, self.hidden_size, bias=self.bias)\n",
        "        self.action_linear        = nn.Linear(self.action_size , self.hidden_size, bias=self.bias)\n",
        "        self.positional_encoding  = nn.Parameter(self.generate_positional_encoding(2, self.hidden_size ), requires_grad=False)\n",
        "        self.transformer_layers   = \\\n",
        "        nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                custom_attn(self.hidden_size, self.num_heads),\n",
        "                nn.LayerNorm(self.hidden_size),\n",
        "                nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias),\n",
        "                nn.LayerNorm(self.hidden_size)\n",
        "            ])\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "        self.reward_linear        = nn.Linear(self.hidden_size, self.reward_size, bias=self.bias)\n",
        "        self.state_linear_        = nn.Linear(self.hidden_size, self.state_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.init  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.opti.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(reduction='none'),\n",
        "            'binary_crossentropy': torch.nn.BCELoss(reduction='none')\n",
        "        }\n",
        "        self.loss_function_ = losses[self.loss .lower()]\n",
        "\n",
        "\n",
        "    def forward(self, s, a_list):\n",
        "\n",
        "        mask = None\n",
        "\n",
        "        r_list = list()\n",
        "        s_list = list()\n",
        "\n",
        "        s  = self.state_linear(s)\n",
        "        s  = self.hidden_activation(s)\n",
        "\n",
        "        a_ = self.action_linear(a_list[:,0])\n",
        "        a_ = self.hidden_activation(a_)\n",
        "\n",
        "        h  = torch.stack([s, a_], dim=0).view(a_.size(0), 2, a_.size(1))\n",
        "        h  = h + self.positional_encoding[:, :, :]\n",
        "\n",
        "        pres_h_list = list()\n",
        "        for j, layer in enumerate(self.transformer_layers):\n",
        "            attention_layer, attention_norm_layer, fully_connected_layer, fully_connected_norm_layer = layer\n",
        "            h_ = attention_layer(h, h, h, mask)\n",
        "            h  = attention_norm_layer(h + h_)\n",
        "            h_ = fully_connected_layer(h)\n",
        "            h  = fully_connected_norm_layer(h + h_)\n",
        "            pres_h_list.append(h)\n",
        "        prev_h_list = pres_h_list\n",
        "\n",
        "        r  = h[:, 0]\n",
        "        s_ = h[:, 1]\n",
        "\n",
        "        r  = self.reward_linear(r)\n",
        "        r  = self.custom_activation(r)\n",
        "\n",
        "        s_ = self.state_linear_(s_)\n",
        "        s_ = self.output_activation(s_)\n",
        "\n",
        "        s  = self.state_linear(s_)\n",
        "        s  = self.hidden_activation(s)\n",
        "\n",
        "        r_list.append(r)\n",
        "        s_list.append(s_)\n",
        "\n",
        "        for i in range(a_list.size(1)-1):\n",
        "\n",
        "            a_ = self.action_linear(a_list[:,i+1])\n",
        "            a_ = self.hidden_activation(a_)\n",
        "\n",
        "            h  = torch.stack([s, a_], dim=0).view(a_.size(0), 2, a_.size(1))\n",
        "            h  = h + self.positional_encoding[:, :, :]\n",
        "\n",
        "            pres_h_list = list()\n",
        "            for j, layer in enumerate(self.transformer_layers):\n",
        "                attention_layer, attention_norm_layer, fully_connected_layer, fully_connected_norm_layer = layer\n",
        "                h_ = attention_layer(prev_h_list[j], prev_h_list[j], h, mask)\n",
        "                h  = attention_norm_layer(h + h_)\n",
        "                h_ = fully_connected_layer(h)\n",
        "                h  = fully_connected_norm_layer(h + h_)\n",
        "                pres_h_list.append(h)\n",
        "            prev_h_list = pres_h_list\n",
        "\n",
        "            r  = h[:, 0]\n",
        "            s_ = h[:, 1]\n",
        "\n",
        "            r  = self.reward_linear(r)\n",
        "            r  = self.custom_activation(r)\n",
        "\n",
        "            s_ = self.state_linear_(s_)\n",
        "            s_ = self.output_activation(s_)\n",
        "\n",
        "            s  = self.state_linear(s_)\n",
        "            s  = self.hidden_activation(s)\n",
        "\n",
        "            r_list.append(r)\n",
        "            s_list.append(s_)\n",
        "\n",
        "        r_list = torch.stack(r_list, dim=1)\n",
        "        s_list = torch.stack(s_list, dim=1)\n",
        "\n",
        "        return r_list, s_list\n",
        "\n",
        "\n",
        "    def generate_positional_encoding(self, max_len, model_dim):\n",
        "        pe = torch.zeros(max_len,model_dim)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0,model_dim,2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/model_dim)))\n",
        "                if i + 1 < model_dim:\n",
        "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/model_dim)))\n",
        "        return pe.unsqueeze(0)  # Shape: (1, max_len, model_dim)\n",
        "\n",
        "    def custom_activation(self, x):\n",
        "        return torch.sigmoid(x + 1.5)\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYsyx03Sxd_"
      },
      "source": [
        "# Function for updating pre-activated action using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_pre_activated_future_action(epoch_for_deducing,\n",
        "                                       model_loader,\n",
        "                                       state,\n",
        "                                       pre_activated_future_action,\n",
        "                                       desired_future_reward,\n",
        "                                       beta):\n",
        "\n",
        "    model_loader_copy = copy.deepcopy(model_loader)\n",
        "\n",
        "    for epoch in range(epoch_for_deducing):\n",
        "\n",
        "        random.shuffle(model_loader_copy)\n",
        "\n",
        "        for model in model_loader_copy:\n",
        "\n",
        "            future_action = torch.sigmoid(pre_activated_future_action)\n",
        "\n",
        "            model.train()\n",
        "            future_action = future_action.clone().detach().requires_grad_(True)\n",
        "            if future_action.grad is not None:\n",
        "                future_action.grad.zero_()\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            loss_function       = model.loss_function\n",
        "            output_reward, _    = model(state, future_action)\n",
        "            total_loss          = loss_function(output_reward, desired_future_reward)\n",
        "            total_loss.backward() # get grad\n",
        "\n",
        "            pre_activated_future_action -= future_action.grad * (1 - future_action) * future_action * beta # update params\n",
        "\n",
        "    return pre_activated_future_action\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yskW6bb1SxeA"
      },
      "source": [
        "# Function for updating model using error backprop\n",
        "\n",
        "Elastic weight consolidation:\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rLQCQigdSxeA"
      },
      "outputs": [],
      "source": [
        "# traditional EWC\n",
        "def EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix):\n",
        "    model_param      = model.state_dict()\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        diagonal_fisher_matrix = prev_gradient_matrix[name] ** 2\n",
        "        param_diff             = (model_param[name] - prev_model_param[name]) ** 2\n",
        "        loss                  += (diagonal_fisher_matrix * param_diff).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_model(epoch_for_learning,\n",
        "                 model,\n",
        "                 train_loader,\n",
        "                 train_loader_,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 EWC_lambda):\n",
        "\n",
        "    for epoch in range(epoch_for_learning):\n",
        "\n",
        "        for state, future_action, future_reward, future_state in train_loader:\n",
        "\n",
        "            model.train()\n",
        "            selected_optimizer = model.selected_optimizer\n",
        "            selected_optimizer.zero_grad()\n",
        "\n",
        "            loss_function               = model.loss_function\n",
        "            output_reward, output_state = model(state, future_action)\n",
        "            total_loss                  = loss_function(output_reward, future_reward) + loss_function(output_state, future_state)\n",
        "            total_loss                 += EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix)\n",
        "            total_loss.backward()     # get grad\n",
        "\n",
        "            selected_optimizer.step() # update params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # training and updating present gradient_matrix\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for state, future_action, future_reward, future_state in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function               = model.loss_function\n",
        "        output_reward, output_state = model(state, future_action)\n",
        "        total_loss                  = loss_function(output_reward, future_reward) + loss_function(output_state, future_state)\n",
        "        total_loss.backward()        # get grad\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if name != \"positional_encoding\":\n",
        "                gradient_matrix[name] += param.grad\n",
        "\n",
        "    gradient_matrix = {name: param / len(train_loader) for name, param in gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return model, gradient_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrw4zh7SxeB"
      },
      "source": [
        "# Function for re-initializing action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_pre_activated_future_action(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xEd6FON1Kr9"
      },
      "source": [
        "# Function for vectorizing\n",
        "Crucial function regarding how you manipulate or shape your state, action and reward\n",
        "\n",
        "- It's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "- As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qivd9tQb1Kr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def vectorizing_state(state):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0 = quantifying(100, -1.5, 0.03 * 1, state[0])\n",
        "    state_1 = quantifying(100, -1.5, 0.03 * 1, state[1])\n",
        "    state_2 = quantifying(100, -1.5, 0.03 * 1, state[2])\n",
        "    state_3 = quantifying(100, -1.5, 0.03 * 1, state[3])\n",
        "    state_4 = quantifying(100, -1, 0.02 * 1, state[4])\n",
        "    state_5 = quantifying(100, -1, 0.02 * 1, state[5])\n",
        "    state_6 = quantifying(100, 0, 0.01 * 1, state[6])\n",
        "    state_7 = quantifying(100, 0, 0.01 * 1, state[7])\n",
        "    # state_10 = np.mean(np.array(env.render(mode='rgb_array')), axis=2, keepdims=True).flatten() / 255\n",
        "    state   = np.atleast_2d(np.concatenate((state_0, state_1, state_2, state_3, state_4, state_5, state_6, state_7)))\n",
        "    return state\n",
        "\n",
        "def vectorizing_action(action_size, action_argmax):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    return np.eye(action_size)[action_argmax]\n",
        "\n",
        "def vectorizing_reward(state, reward, summed_reward, reward_size):       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    reward = quantifying(reward_size, -400, (350 - (-400))/reward_size, reward)\n",
        "    return reward\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENsHGddSxeC"
      },
      "source": [
        "# Function for sequentializing state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XUFHTgan1Kr_"
      },
      "outputs": [],
      "source": [
        "def sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size):\n",
        "\n",
        "    short_term_present_state_list = []\n",
        "    short_term_future_action_list = []\n",
        "    short_term_future_reward_list = []\n",
        "    short_term_future_state_list  = []\n",
        "\n",
        "    if time_size > len(short_term_state_list[:-1]):\n",
        "        time_size = len(short_term_state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    for i in range(len(short_term_reward_list[:-time_size+1])):\n",
        "        short_term_present_state_list.append(      short_term_state_list [ i                       ]  )\n",
        "        short_term_future_action_list.append(      short_term_action_list[ i   : i+time_size       ]  )\n",
        "        short_term_future_reward_list.append(      short_term_reward_list[ i   : i+time_size       ]  )\n",
        "        short_term_future_state_list.append(       short_term_state_list [ i+1 : i+time_size+1     ]  )\n",
        "\n",
        "    return short_term_present_state_list, short_term_future_action_list, short_term_future_reward_list, short_term_future_state_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTosZ2u01xa"
      },
      "source": [
        "# Function for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1zJs9Dgv01xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_from_list(short_term_state_list,\n",
        "                            short_term_future_action_list,\n",
        "                            short_term_future_reward_list,\n",
        "                            short_term_future_state_list,\n",
        "                            device):\n",
        "\n",
        "    # Convert lists to tensors directly on the desired device and data type\n",
        "    short_term_state_tensor         = torch.tensor(np.array(short_term_state_list), dtype=torch.float).to(device)\n",
        "    short_term_future_action_tensor = torch.tensor(np.array(short_term_future_action_list), dtype=torch.float).to(device)\n",
        "    short_term_future_reward_tensor = torch.tensor(np.array(short_term_future_reward_list), dtype=torch.float).to(device)\n",
        "    short_term_future_state_tensor  = torch.tensor(np.array(short_term_future_state_list), dtype=torch.float).to(device)\n",
        "\n",
        "    return short_term_state_tensor, short_term_future_action_tensor, short_term_future_reward_tensor, short_term_future_state_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N5YNnDY61KsA"
      },
      "outputs": [],
      "source": [
        "def obtain_TD_error(model,\n",
        "                    train_loader_,\n",
        "                    device):\n",
        "\n",
        "\n",
        "    for state, future_action, future_reward, future_state in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function_\n",
        "        output_reward, _     = model(state, future_action)\n",
        "        total_loss           = loss_function(output_reward, future_reward).detach().to(device)\n",
        "        total_loss           = torch.sum(torch.abs(total_loss), dim=(1, 2))\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tYxaYpK601xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_according_to_TD_error(state_tensor, future_action_tensor, future_reward_tensor, future_state_tensor, index_list):\n",
        "\n",
        "    # Use advanced indexing to select elements based on random_index\n",
        "    state_tensor          = state_tensor[index_list]\n",
        "    future_action_tensor  = future_action_tensor[index_list]\n",
        "    future_reward_tensor  = future_reward_tensor[index_list]\n",
        "    future_state_tensor   = future_state_tensor[index_list]\n",
        "\n",
        "    return state_tensor, future_action_tensor, future_reward_tensor, future_state_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mzAXnMNU1KsB"
      },
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"               # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "max_steps_for_each_episode = 200           # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "ensemble_size = 10                # choose the size of the neural ensemble (Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️)\n",
        "state_size =  800                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "action_size = 4                   # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "reward_size = 250                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "hidden_size = 250                 # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "time_size = 50                    # (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "neural_type = 'att'               # choose your neural type: [rnn gru lstm] [att]\n",
        "num_layers = 2                    # choose the number of layers for your rnn or attention: [1 2 3 4 etc.]\n",
        "num_heads = 10                    # choose your number of heads: [None for non-attention] [should be able to divide hidden_size for attention]\n",
        "hidden_activation = 'tanh'        # choose hidden activation function: [relu leaky_relu sigmoid tanh]\n",
        "output_activation = 'sigmoid'     # choose output activation function: [relu leaky_relu sigmoid tanh]\n",
        "init = \"random_normal\"            # choose initialization method: [random_normal random_uniform xavier_normal xavier_uniform glorot_normal glorot_uniform]\n",
        "opti = 'sgd'                      # choose optimization method: [adam sgd rmsprop]\n",
        "loss = 'mean_squared_error'       # choose error function type: [mean_squared_error binary_crossentropy]\n",
        "drop_rate = 0.0001                # choose your drop rate (Reminder: change this value to see the impact of drop-out ◀️◀️◀️)\n",
        "alpha = 0.1                       # choose your learning rate for updating neural nets\n",
        "epoch_for_learning = 10           # choose learning epoch for nn weights (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "batch_size = 1                    # batch_size for learning\n",
        "load_pre_model = False            # retrain from existing neural nets or not\n",
        "\n",
        "\n",
        "noise_t = 1               # gaussian noise\n",
        "noise_r = 0.1             # smaller value encourages agent to exploit experience while larger value encourages agent to explore at the cost of longer training time\n",
        "beta = 0.1                # updating rate for input action\n",
        "epoch_for_deducing =  int(100/ensemble_size)    # updating epoch for input action (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 1     # batch size for batch offline learning (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "PER_sample_size = 1000                   # prioritized experience replay samples (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "EWC_lambda = 1                           # elastic weight control lambda (Reminder: change this for your specific task ⚠️⚠️⚠️)\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={epoch_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={epoch_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            action_size,\n",
        "                            reward_size,\n",
        "                            hidden_size,\n",
        "                            time_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            action_size,\n",
        "                            reward_size,\n",
        "                            hidden_size,\n",
        "                            time_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulHSk5_SxeG"
      },
      "source": [
        "Creating intial gradient matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_future_reward = torch.ones((batch_size, time_size, reward_size)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "84b5d12c-7a31-4216-9aaf-58edabdfd054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env           = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state         = env.reset()\n",
        "    summed_step   = 0\n",
        "    summed_reward = 0\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state)\n",
        "    short_term_state_list.append(state[0])\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "\n",
        "\n",
        "        # initializing and updating action\n",
        "        state                       = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = initialize_pre_activated_future_action(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_future_action = torch.tensor(pre_activated_future_action, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = update_pre_activated_future_action(epoch_for_deducing,\n",
        "                                                                  model_loader,\n",
        "                                                                  state,\n",
        "                                                                  pre_activated_future_action,\n",
        "                                                                  desired_future_reward,\n",
        "                                                                  beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_future_action[0, 0]))\n",
        "        action           = vectorizing_action(action_size, action_argmax)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_argmax)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_step += 1\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state)\n",
        "        short_term_state_list.append(state[0])\n",
        "\n",
        "\n",
        "        if summed_step < time_size:\n",
        "            done = False\n",
        "        else:\n",
        "            if done:\n",
        "                print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "                performance_log.append([training_episode+1, summed_reward])\n",
        "                save_performance_to_csv(performance_log, performance_log_directory)\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentialize short term experience replay buffer and then save it to long term experience replay buffer\n",
        "    short_term_state_list, \\\n",
        "    short_term_future_action_list, \\\n",
        "    short_term_future_reward_list, \\\n",
        "    short_term_future_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size )\n",
        "    short_term_state_tensor,\\\n",
        "    short_term_future_action_tensor,\\\n",
        "    short_term_future_reward_tensor,\\\n",
        "    short_term_future_state_tensor = obtain_tensor_from_list(short_term_state_list,\n",
        "                                                             short_term_future_action_list,\n",
        "                                                             short_term_future_reward_list,\n",
        "                                                             short_term_future_state_list,\n",
        "                                                             device)\n",
        "    if training_episode==0:\n",
        "        long_term_state_tensor               = copy.deepcopy(short_term_state_tensor)\n",
        "        long_term_future_action_tensor       = copy.deepcopy(short_term_future_action_tensor)\n",
        "        long_term_future_reward_tensor       = copy.deepcopy(short_term_future_reward_tensor)\n",
        "        long_term_future_state_tensor        = copy.deepcopy(short_term_future_state_tensor)\n",
        "    else:\n",
        "        long_term_state_tensor               = torch.cat((long_term_state_tensor              , short_term_state_tensor            ), dim=0)\n",
        "        long_term_future_action_tensor       = torch.cat((long_term_future_action_tensor      , short_term_future_action_tensor    ), dim=0)\n",
        "        long_term_future_reward_tensor       = torch.cat((long_term_future_reward_tensor      , short_term_future_reward_tensor    ), dim=0)\n",
        "        long_term_future_state_tensor        = torch.cat((long_term_future_state_tensor       , short_term_future_state_tensor     ), dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        gradient_matrix_loader = [''] * len(model_loader)\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # creating samples for prioritized experience replay buffer:\n",
        "                dataset      = TensorDataset(long_term_state_tensor,\n",
        "                                             long_term_future_action_tensor,\n",
        "                                             long_term_future_reward_tensor,\n",
        "                                             long_term_future_state_tensor)\n",
        "                data_loader_ = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "                total_temporal_difference_error = 0\n",
        "                temporal_difference_error        = obtain_TD_error(model,\n",
        "                                                                   data_loader_,\n",
        "                                                                   device)\n",
        "                total_temporal_difference_error += temporal_difference_error\n",
        "                total_temporal_difference_error = total_temporal_difference_error.cpu().numpy()\n",
        "                total_temporal_difference_error = total_temporal_difference_error / np.sum(total_temporal_difference_error)\n",
        "                index_list                      = np.random.choice(range(len(dataset)),\n",
        "                                                                   size=PER_sample_size,\n",
        "                                                                   p=total_temporal_difference_error,\n",
        "                                                                   replace=True)\n",
        "                selected_state_tensor,\\\n",
        "                selected_action_tensor,\\\n",
        "                selected_reward_tensor,\\\n",
        "                selected_future_state_tensor = obtain_tensor_according_to_TD_error(long_term_state_tensor,\n",
        "                                                                                   long_term_future_action_tensor,\n",
        "                                                                                   long_term_future_reward_tensor,\n",
        "                                                                                   long_term_future_state_tensor,\n",
        "                                                                                   index_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # starting learning with elastic weight control\n",
        "                dataset      = TensorDataset(selected_state_tensor,\n",
        "                                             selected_action_tensor,\n",
        "                                             selected_reward_tensor,\n",
        "                                             selected_future_state_tensor)\n",
        "                data_loader  = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
        "                data_loader_ = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "                model, gradient_matrix    = update_model(epoch_for_learning,\n",
        "                                                         model,\n",
        "                                                         data_loader,\n",
        "                                                         data_loader_,\n",
        "                                                         prev_model_loader[i],\n",
        "                                                         prev_gradient_matrix_loader[i],\n",
        "                                                         EWC_lambda)\n",
        "                model_loader[i]           = model\n",
        "                gradient_matrix_loader[i] = gradient_matrix\n",
        "        torch.cuda.synchronize()\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_gradient_matrix_loader = copy.deepcopy(gradient_matrix_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        hidden_size,\n",
        "                        time_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_future_reward = torch.ones((batch_size, time_size, reward_size)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                       = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = initialize_pre_activated_future_action(init, noise_t, noise_r, (1, time_size, action_size))\n",
        "        pre_activated_future_action = torch.tensor(pre_activated_future_action, dtype=torch.float).to(device)\n",
        "        pre_activated_future_action = update_pre_activated_future_action(epoch_for_deducing,\n",
        "                                                                  model_loader,\n",
        "                                                                  state,\n",
        "                                                                  pre_activated_future_action,\n",
        "                                                                  desired_future_reward,\n",
        "                                                                  beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_future_action[0, 0]))\n",
        "\n",
        "        state, reward, done,  info = env.step(action_argmax)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}