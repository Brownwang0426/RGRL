{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "3350c060-7302-4eeb-bfc9-f73f03974e0e"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0y9RfWif-v2",
        "outputId": "31b7bb14-f74e-4aaf-fe69-4cebbdb8bbcf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "af231b3d-b797-492f-b1ad-2d5ce1b94ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA T500\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYykFEEVSxd9"
      },
      "source": [
        "# Class for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rZC3T9IXZDP"
      },
      "outputs": [],
      "source": [
        "class build_model(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_neuron_size_,\n",
        "                 hidden_neuron_size,\n",
        "                 input_neuron_size,\n",
        "                 input_sequence_size,\n",
        "                 output_neuron_size,\n",
        "                 neural_type,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_activation,\n",
        "                 output_activation,\n",
        "                 initializer,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 drop_rate,\n",
        "                 alpha,\n",
        "                 mask_value):\n",
        "\n",
        "        super(build_model, self).__init__()\n",
        "\n",
        "        self.input_neuron_size_   = int(input_neuron_size_)\n",
        "        self.hidden_neuron_size   = int(hidden_neuron_size)\n",
        "        self.input_neuron_size    = int(input_neuron_size)\n",
        "        self.input_sequence_size  = int(input_sequence_size)\n",
        "        self.output_neuron_size   = int(output_neuron_size)\n",
        "        self.neural_type          = neural_type\n",
        "        self.num_heads            = num_heads\n",
        "\n",
        "        self.hidden_activation    = hidden_activation\n",
        "        self.output_activation    = output_activation\n",
        "        self.initializer          = initializer\n",
        "        self.optimizer            = optimizer\n",
        "        self.loss                 = loss\n",
        "        self.drop_rate            = drop_rate\n",
        "        self.alpha                = alpha\n",
        "        self.mask_value           = mask_value\n",
        "\n",
        "        self.bias = False\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        neural_types = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM\n",
        "        }\n",
        "\n",
        "        self.fully_connected_layer_in_0      = nn.Linear(self.input_neuron_size_, self.hidden_neuron_size, bias=self.bias)\n",
        "        self.fully_connected_layer_in_1      = nn.Linear(self.hidden_neuron_size, self.hidden_neuron_size, bias=self.bias)\n",
        "\n",
        "        self.recurrent_layer_0               = neural_types[neural_type.lower()](self.input_neuron_size, self.hidden_neuron_size, num_layers=self.num_layers, batch_first=False, bias=self.bias, dropout=self.drop_rate)\n",
        "\n",
        "        self.fully_connected_layer_0         = nn.Linear(self.hidden_neuron_size * self.input_sequence_size, self.output_neuron_size, bias=self.bias)\n",
        "\n",
        "        # Activation functions\n",
        "        self.hidden_activation = self.get_activation(self.hidden_activation)\n",
        "        self.output_activation = self.get_activation(self.output_activation)\n",
        "\n",
        "        # Initialize weights for fully connected layers\n",
        "        self.initialize_weights(self.initializer  )\n",
        "\n",
        "        # Optimizer\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'sgd': optim.SGD,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.selected_optimizer = optimizers[self.optimizer.lower()](self.parameters(), lr=self.alpha)\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(),\n",
        "            'binary_crossentropy': torch.nn.BCELoss()\n",
        "        }\n",
        "        self.loss_function = losses[self.loss .lower()]\n",
        "\n",
        "        # Loss function\n",
        "        losses = {\n",
        "            'mean_squared_error': torch.nn.MSELoss(reduction='none'),\n",
        "            'binary_crossentropy': torch.nn.BCELoss(reduction='none')\n",
        "        }\n",
        "        self.loss_function_ = losses[self.loss .lower()]\n",
        "\n",
        "\n",
        "    def forward(self, initial_hidden, x, padding_mask):\n",
        "\n",
        "        h  = self.fully_connected_layer_in_0(initial_hidden)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = self.fully_connected_layer_in_1(h)\n",
        "        h  = self.hidden_activation(h)\n",
        "        h  = torch.unsqueeze(h, dim=0).repeat(self.num_layers, 1, 1)\n",
        "\n",
        "        out        = x.permute(1, 0, 2)\n",
        "        lengths    = (out != self.mask_value).any(dim=2).sum(dim=0).cpu().long() # since x is (sequence_length, batch_size, input_size), we should use sum(dim=0)\n",
        "        out        = rnn_utils.pack_padded_sequence(out, lengths, batch_first=False, enforce_sorted=False)\n",
        "        # Forward propagate RNN\n",
        "        if self.neural_type == 'lstm':\n",
        "            out, h   = self.recurrent_layer_0(out, (h, h))\n",
        "        else:\n",
        "            out, h   = self.recurrent_layer_0(out, h)\n",
        "\n",
        "        out, _     = rnn_utils.pad_packed_sequence(out, batch_first=False)\n",
        "        padding    = (0, 0, 0, 0, 0, self.input_sequence_size - out.size(0))\n",
        "        out        = F.pad(out, padding, \"constant\", 0)\n",
        "        out        = out.permute(1, 0, 2)\n",
        "\n",
        "        out    = torch.flatten(out, start_dim=1)\n",
        "        out    = self.fully_connected_layer_0(out)\n",
        "        out    = self.output_activation(out)\n",
        "\n",
        "        return out \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_activation(self,  activation):\n",
        "        activations = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'leaky_relu': nn.LeakyReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        return activations[ activation.lower()]\n",
        "\n",
        "    def initialize_weights(self, initializer):\n",
        "        initializers = {\n",
        "            'random_uniform': nn.init.uniform_,\n",
        "            'random_normal': nn.init.normal_,\n",
        "            'glorot_uniform': nn.init.xavier_uniform_,\n",
        "            'glorot_normal': nn.init.xavier_normal_,\n",
        "            'xavier_uniform': nn.init.xavier_uniform_,\n",
        "            'xavier_normal': nn.init.xavier_normal_\n",
        "        }\n",
        "        initializer = initializers[initializer.lower()]\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                initializer(layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYsyx03Sxd_"
      },
      "source": [
        "# Function for updating pre-activated actions using error backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibjd5YRvpQPL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_pre_activated_actions(epoch_for_deducing,\n",
        "                                 model_loader,\n",
        "                                 state,\n",
        "                                 pre_activated_actions,\n",
        "                                 desired_reward,\n",
        "                                 beta):\n",
        "\n",
        "    model_loader_copy = copy.deepcopy(model_loader)\n",
        "\n",
        "    for epoch in range(epoch_for_deducing):\n",
        "\n",
        "        random.shuffle(model_loader_copy)\n",
        "\n",
        "        for model in model_loader_copy:\n",
        "\n",
        "            actions = torch.sigmoid(pre_activated_actions)\n",
        "\n",
        "            model.train()\n",
        "            actions = actions.clone().detach().requires_grad_(True)\n",
        "            if actions.grad is not None:\n",
        "                actions.grad.zero_()\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            loss_function = model.loss_function\n",
        "            output        = model(state, actions, padding_mask=None)\n",
        "            total_loss    = loss_function(output, desired_reward)\n",
        "            total_loss.backward() # get grad\n",
        "\n",
        "            pre_activated_actions -= actions.grad * (1 - actions) * actions * beta # update params\n",
        "\n",
        "    return pre_activated_actions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yskW6bb1SxeA"
      },
      "source": [
        "# Function for updating model using error backprop\n",
        "\n",
        "Elastic weight consolidation:\n",
        "https://arxiv.org/pdf/1612.00796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rLQCQigdSxeA"
      },
      "outputs": [],
      "source": [
        "# traditional EWC\n",
        "def EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix):\n",
        "    model_param      = model.state_dict()\n",
        "    prev_model_param = prev_model.state_dict()\n",
        "    loss = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        diagonal_fisher_matrix = prev_gradient_matrix[name] ** 2\n",
        "        param_diff             = (model_param[name] - prev_model_param[name]) ** 2\n",
        "        loss                  += (diagonal_fisher_matrix * param_diff).sum()\n",
        "    return EWC_lambda * loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_model(epoch_for_learning,\n",
        "                 model,\n",
        "                 train_loader,\n",
        "                 train_loader_,\n",
        "                 prev_model,\n",
        "                 prev_gradient_matrix,\n",
        "                 EWC_lambda):\n",
        "\n",
        "    for epoch in range(epoch_for_learning):\n",
        "\n",
        "        for state, actions, reward, next_state, padding_mask in train_loader:\n",
        "\n",
        "            model.train()\n",
        "            selected_optimizer = model.selected_optimizer\n",
        "            selected_optimizer.zero_grad()\n",
        "\n",
        "            loss_function        = model.loss_function\n",
        "            output               = model(state, actions, padding_mask)\n",
        "            total_loss           = loss_function(output, reward) \n",
        "            total_loss          += EWC_loss(EWC_lambda, model, prev_model, prev_gradient_matrix)\n",
        "            total_loss.backward()     # get grad\n",
        "\n",
        "            selected_optimizer.step() # update params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # training and updating present gradient_matrix\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "\n",
        "    for state, actions, reward, next_state, padding_mask in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function\n",
        "        output               = model(state, actions, padding_mask)\n",
        "        total_loss           = loss_function(output, reward) \n",
        "        total_loss.backward()        # get grad\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            gradient_matrix[name] += param.grad\n",
        "\n",
        "    gradient_matrix = {name: param / len(train_loader) for name, param in gradient_matrix.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return model, gradient_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrw4zh7SxeB"
      },
      "source": [
        "# Function for re-initializing action value in each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PlPeg47KB9u4"
      },
      "outputs": [],
      "source": [
        "def initialize_pre_activated_actions(init, noise_t, noise_r, shape):\n",
        "    input = 0\n",
        "    if   init == \"random_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.uniform(low=0, high=1, size=shape)    ]) * noise_r\n",
        "    elif init == \"random_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= 1, size= shape )    ])  * noise_r\n",
        "    elif init == \"glorot_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"glorot_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    elif init == \"xavier_uniform\":\n",
        "        for _ in range(noise_t):\n",
        "            limit = np.sqrt(6 / (shape[1] + shape[1]))\n",
        "            input += np.array([  np.random.uniform(low=-limit, high=limit, size=shape)    ])  * noise_r\n",
        "    elif init == \"xavier_normal\":\n",
        "        for _ in range(noise_t):\n",
        "            input += np.array([  np.random.normal(loc=0.0, scale= np.sqrt(2 / (shape[1] + shape[1])) , size= shape )    ])  * noise_r\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function for vectorizing\n",
        "Crucial function regarding how you manipulate or shape your state, action and reward\n",
        "\n",
        "- It's essential to choose between immediate rewards and summed rewards for training your agent. If the current state doesn't encapsulate all crucial past information, using immediate rewards is advisable. This approach prevents confusion caused by varying summed rewards for the same state.\n",
        "\n",
        "- As for reward shaping, it is recommended to increase your reward upper and decrease your reward lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def quantifying(array_size, init, interval, input):\n",
        "    array = np.zeros(array_size)\n",
        "    index = int( (input - init) // interval + 1)\n",
        "    if index >= 0:\n",
        "        array[ : index] = 1\n",
        "    return array\n",
        "\n",
        "def vectorizing_state(state):      # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    state_0 = quantifying(100, -2.5, 0.050, state[0])\n",
        "    state_1 = quantifying(100, -3.75, 0.075, state[1])\n",
        "    state_2 = quantifying(100, -0.375, 0.0075, state[2])\n",
        "    state_3 = quantifying(100, -3.75, 0.075, state[3])\n",
        "    state_4 = quantifying(100, 0, 10, 0)\n",
        "    state   = np.atleast_2d(np.concatenate((state_0, state_1, state_2, state_3, state_4)))\n",
        "    return state\n",
        "\n",
        "def vectorizing_action(action_size, action_argmax):  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    return np.eye(action_size)[action_argmax]\n",
        "\n",
        "def vectorizing_reward(state, reward, summed_reward, done, reward_size):       # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "    if done:\n",
        "        reward = np.zeros(reward_size)\n",
        "    else:\n",
        "        reward = np.ones(reward_size)\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENsHGddSxeC"
      },
      "source": [
        "# Function for sequentializing state, action and reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I don't know why the following multi-processing does not work...T_T (I suck) But I kept it just for later investigation.\n",
        "\n",
        "def process_time_size(params):\n",
        "    state_list, action_list, reward_list, time_size, time = params\n",
        "\n",
        "    sequentialized_state_list      = []\n",
        "    sequentialized_action_list     = []\n",
        "    sequentialized_reward_list     = []\n",
        "    sequentialized_next_state_list = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    time_size_ = time_size\n",
        "    for i in range(len(reward_list[:])):\n",
        "        sequentialized_state_list.append(       state_list [ i ] )\n",
        "        sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "        sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "        sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    # a more sophisticated method\n",
        "    # for j in range(time_size):\n",
        "    #     time_size_ = j+1\n",
        "    #     if time_size_== 1:\n",
        "    #         for i in range(len(reward_list[:])):\n",
        "    #             sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #             sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #             sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #             sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "    #     else:\n",
        "    #         for i in range(len(reward_list[:-time_size_+1])):\n",
        "    #             sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #             sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #             sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #             sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    return (sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "    # Prepare the parameters for each process\n",
        "    params_list = [(state_list, action_list, reward_list, time_size, time) for time in range(time_size)]\n",
        "\n",
        "    # Use multiprocessing Pool to process chunks in parallel\n",
        "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
        "        results = pool.map(process_time_size, params_list)\n",
        "\n",
        "    # Aggregate results\n",
        "    sequentialized_state_list = []\n",
        "    sequentialized_action_list = []\n",
        "    sequentialized_reward_list = []\n",
        "    sequentialized_next_state_list = []\n",
        "\n",
        "    for result in results:\n",
        "        s_states, s_actions, s_rewards, s_next_states = result\n",
        "        sequentialized_state_list.extend(s_states)\n",
        "        sequentialized_action_list.extend(s_actions)\n",
        "        sequentialized_reward_list.extend(s_rewards)\n",
        "        sequentialized_next_state_list.extend(s_next_states)\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sequentialize(state_list, action_list, reward_list, time_size):\n",
        "\n",
        "    sequentialized_state_list       = []\n",
        "    sequentialized_action_list      = []\n",
        "    sequentialized_reward_list      = []\n",
        "    sequentialized_next_state_list  = []\n",
        "\n",
        "    if time_size > len(state_list[:-1]):\n",
        "        time_size = len(state_list[:-1])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    time_size_ = time_size\n",
        "    for i in range(len(reward_list[:])):\n",
        "        sequentialized_state_list.append(       state_list [ i ] )\n",
        "        sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "        sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "        sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    # a more sophisticated method\n",
        "    # for j in range(time_size):\n",
        "    #     time_size_ = j+1\n",
        "    #     if time_size_== 1:\n",
        "    #         for i in range(len(reward_list[:])):\n",
        "    #             sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #             sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #             sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #             sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "    #     else:\n",
        "    #         for i in range(len(reward_list[:-time_size_+1])):\n",
        "    #             sequentialized_state_list.append(       state_list [ i ] )\n",
        "    #             sequentialized_action_list.append(      action_list[ i : i+time_size_]  )\n",
        "    #             sequentialized_reward_list.append(      reward_list[ i + len(action_list[i:i+time_size_]) - 1 ]  )\n",
        "    #             sequentialized_next_state_list.append(  state_list [ i + len(action_list[i:i+time_size_])     ]  )\n",
        "\n",
        "    return sequentialized_state_list, sequentialized_action_list, sequentialized_reward_list, sequentialized_next_state_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTosZ2u01xa"
      },
      "source": [
        "# Function for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1zJs9Dgv01xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_from_list(sequentialized_state_list,\n",
        "                            sequentialized_actions_list,\n",
        "                            sequentialized_reward_list,\n",
        "                            sequentialized_next_state_list,\n",
        "                            device,\n",
        "                            time_size,\n",
        "                            mask_value,\n",
        "                            num_heads):\n",
        "\n",
        "    # Convert lists to tensors directly on the desired device and data type\n",
        "    state_tensor = torch.tensor(np.array(sequentialized_state_list), dtype=torch.float).to(device)\n",
        "    reward_tensor = torch.tensor(np.array(sequentialized_reward_list), dtype=torch.float).to(device)\n",
        "    next_state_tensor = torch.tensor(np.array(sequentialized_next_state_list), dtype=torch.float).to(device)\n",
        "\n",
        "    # Pad and stack actions_tensor efficiently\n",
        "    actions_list = []\n",
        "    for arr in sequentialized_actions_list:\n",
        "        tensor_arr = torch.tensor(np.array(arr), dtype=torch.float).to(device)\n",
        "        # Pad tensor only once per tensor\n",
        "        if tensor_arr.size(0) < time_size:\n",
        "            padded_arr = F.pad(tensor_arr,\n",
        "                               (0, 0, 0, time_size - tensor_arr.size(0)),\n",
        "                               mode='constant',\n",
        "                               value=mask_value)\n",
        "        else:\n",
        "            padded_arr = tensor_arr\n",
        "        actions_list.append(padded_arr)\n",
        "    actions_tensor = torch.stack(actions_list).to(device)\n",
        "\n",
        "    # Compute row_mask and padding_mask efficiently\n",
        "    row_mask = (actions_tensor == mask_value).all(dim=-1)\n",
        "    padding_mask = row_mask.to(dtype=torch.bool)\n",
        "    padding_mask = padding_mask.to(device)\n",
        "\n",
        "    return state_tensor, actions_tensor, reward_tensor, next_state_tensor, padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_error(model,\n",
        "                 train_loader_,\n",
        "                 device):\n",
        "\n",
        "\n",
        "    for state, actions, reward, next_state, padding_mask in train_loader_:\n",
        "\n",
        "        model.train()\n",
        "        selected_optimizer = model.selected_optimizer\n",
        "        selected_optimizer.zero_grad()\n",
        "\n",
        "        loss_function        = model.loss_function_\n",
        "        output               = model(state, actions, padding_mask)\n",
        "        total_loss           = loss_function(output, reward).detach().to(device)\n",
        "        total_loss           = torch.sum(torch.abs(total_loss), axis=1)\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tYxaYpK601xa"
      },
      "outputs": [],
      "source": [
        "def obtain_tensor_according_to_TD_error(state_tensor, actions_tensor, reward_tensor, next_state_tensor, padding_mask, index_list):\n",
        "\n",
        "    # Use advanced indexing to select elements based on random_index\n",
        "    state_tensor = state_tensor[index_list]\n",
        "    actions_tensor = actions_tensor[index_list]\n",
        "    reward_tensor = reward_tensor[index_list]\n",
        "    next_state_tensor = next_state_tensor[index_list]\n",
        "    padding_mask = padding_mask[index_list]\n",
        "\n",
        "    return state_tensor, actions_tensor, reward_tensor, next_state_tensor, padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_performance_to_csv(performance_log, filename='performance_log.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Episode', 'Summed_Reward'])\n",
        "        writer.writerows(performance_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial variables regarding how your agent will learn in the environment\n",
        "\n",
        "- In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-wKwjw13ftNU"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'                # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "max_steps_for_each_episode = 2000        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                        # Reminder: change this value to see the impact of MWM-SGD ◀️◀️◀️\n",
        "state_size =  500                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "hidden_size = 100                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "action_size = 2                          # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "time_size = 15                           # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "reward_size = 100                        # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "neural_type = 'gru'                      # rnn gru lstm; att\n",
        "num_layers = 2                           # 1, 2, 3, 4, etc.\n",
        "num_heads = None                         # None for non-attention; should be able to divide hidden_size for attention\n",
        "hidden_activation = 'tanh'               # relu leaky_relu sigmoid tanh\n",
        "output_activation = 'sigmoid'            # relu leaky_relu sigmoid tanh\n",
        "\n",
        "init = \"random_normal\"                   # random_normal random_uniform xavier_normal xavier_uniform  glorot_normal  glorot_uniform\n",
        "opti = 'sgd'                             # adam sgd rmsprop\n",
        "loss = 'mean_squared_error'              # mean_squared_error  binary_crossentropy\n",
        "drop_rate = 0.001                       # Reminder: change this value to see the impact of drop-out ◀️◀️◀️\n",
        "alpha = 0.1                              # learning rate\n",
        "epoch_for_learning = 10                  # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "mask_value = sys.maxsize                 # mask value\n",
        "batch_size = 1\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1               # gaussian noise\n",
        "noise_r = 0.1             # smaller value encourages agent to exploit experience while larger value encourages agent to explore\n",
        "beta = 0.1\n",
        "epoch_for_deducing =  int(100/ensemble_size)    # Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "batch_size_for_offline_learning = 10     # batch size for batch offline learning, Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "PER_sample_size = 10000                  # prioritized experience replay samples, Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "EWC_lambda = 1                           # elastic weight control lambda, Reminder: change this for your specific task ⚠️⚠️⚠️\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "\n",
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={epoch_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={epoch_for_deducing:05d}_lambda={EWC_lambda:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing > Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_loader = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            drop_rate,\n",
        "                            alpha,\n",
        "                            mask_value)\n",
        "        model.to(device)\n",
        "        model_loader.append(model)\n",
        "\n",
        "    for i in range(len(model_loader)):\n",
        "        model_loader[i].load_state_dict(torch.load( model_directory  % i ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulHSk5_SxeG"
      },
      "source": [
        "Creating intial gradient matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "81pqjJejwBjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "prev_model_loader = copy.deepcopy(model_loader)\n",
        "\n",
        "prev_gradient_matrix_loader = []\n",
        "for model in model_loader:\n",
        "    gradient_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "    prev_gradient_matrix_loader.append( gradient_matrix )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "df8f8daf-8e05-41fb-dd81-5dc6983bd6ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100000 [00:00<?, ?it/s]c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1142: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:1424.)\n",
            "  result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "c:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "  0%|          | 1/100000 [00:15<440:43:57, 15.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/100000 [00:54<805:28:51, 29.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2: Summed_Reward = 41.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/100000 [01:07<603:09:42, 21.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/100000 [01:14<445:56:00, 16.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/100000 [01:28<428:25:24, 15.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 6/100000 [01:41<398:08:00, 14.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 6: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/100000 [01:49<343:15:47, 12.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 7: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 8/100000 [01:58<314:30:05, 11.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 8: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 9/100000 [02:37<555:23:58, 20.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 9: Summed_Reward = 47.0\n",
            "Episode 10: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 11/100000 [05:14<1267:02:28, 45.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 11: Summed_Reward = 37.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 12/100000 [05:29<1005:23:46, 36.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 12: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 13/100000 [05:38<778:09:28, 28.02s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 13: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 14/100000 [05:51<652:09:26, 23.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 14: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 15/100000 [05:58<511:20:21, 18.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 15: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 16/100000 [06:05<420:25:15, 15.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 16: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 17/100000 [06:12<351:48:42, 12.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 17: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 18/100000 [06:19<300:29:19, 10.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 18: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 19/100000 [06:31<318:28:17, 11.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19: Summed_Reward = 19.0\n",
            "Episode 20: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 21/100000 [09:45<1348:55:29, 48.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 21: Summed_Reward = 43.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 22/100000 [09:54<1018:54:43, 36.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 22: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 23/100000 [10:00<763:42:10, 27.50s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 23: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 24/100000 [10:09<611:47:46, 22.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 24: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 25/100000 [10:22<539:46:50, 19.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 25: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 26/100000 [10:30<442:19:01, 15.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 26: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 27/100000 [10:43<419:11:28, 15.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 27: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 28/100000 [10:53<372:45:29, 13.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 28: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 29/100000 [11:00<323:26:07, 11.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 29: Summed_Reward = 14.0\n",
            "Episode 30: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 31/100000 [14:54<1560:03:07, 56.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 31: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 32/100000 [15:03<1167:14:45, 42.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 32: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 33/100000 [15:18<944:51:39, 34.03s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 33: Summed_Reward = 29.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 34/100000 [15:28<739:43:32, 26.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 34: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 35/100000 [15:35<574:59:58, 20.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 35: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 36/100000 [16:01<624:57:20, 22.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 36: Summed_Reward = 50.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 37/100000 [16:09<499:35:07, 17.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 37: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 38/100000 [16:30<523:02:43, 18.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 38: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 39/100000 [16:52<550:28:54, 19.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 39: Summed_Reward = 42.0\n",
            "Episode 40: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 41/100000 [22:10<2151:42:13, 77.49s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 41: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 42/100000 [22:31<1681:18:17, 60.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 42: Summed_Reward = 34.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 43/100000 [22:48<1315:56:20, 47.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 43: Summed_Reward = 29.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 44/100000 [22:55<984:12:20, 35.45s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 44: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 45/100000 [23:08<797:09:35, 28.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 45: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 46/100000 [23:19<645:14:11, 23.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 46: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 47/100000 [23:29<533:36:06, 19.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 47: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 48/100000 [23:35<425:09:21, 15.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 48: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 49/100000 [23:48<404:16:44, 14.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 49: Summed_Reward = 24.0\n",
            "Episode 50: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 51/100000 [30:20<2557:20:23, 92.11s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 51: Summed_Reward = 47.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 52/100000 [30:36<1922:52:04, 69.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 52: Summed_Reward = 28.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 53/100000 [30:49<1449:53:01, 52.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 53: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 54/100000 [31:11<1197:41:29, 43.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 54: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 55/100000 [31:23<940:22:12, 33.87s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 55: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 56/100000 [31:31<722:06:08, 26.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 56: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 57/100000 [31:58<728:20:09, 26.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 57: Summed_Reward = 51.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 58/100000 [32:13<640:03:28, 23.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 58: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 59/100000 [32:21<514:17:56, 18.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 59: Summed_Reward = 15.0\n",
            "Episode 60: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 61/100000 [40:10<3003:17:51, 108.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 61: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 62/100000 [40:43<2376:42:50, 85.61s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 62: Summed_Reward = 61.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 63/100000 [40:49<1711:14:49, 61.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 63: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 64/100000 [41:00<1291:27:07, 46.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 64: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 65/100000 [41:06<958:12:06, 34.52s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 65: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 66/100000 [41:17<758:48:42, 27.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 66: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 67/100000 [41:24<592:42:14, 21.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 67: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 68/100000 [41:31<469:14:15, 16.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 68: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 69/100000 [41:44<436:54:34, 15.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 69: Summed_Reward = 24.0\n",
            "Episode 70: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 71/100000 [50:31<3304:44:11, 119.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 71: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 72/100000 [50:43<2406:03:32, 86.68s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 72: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 73/100000 [50:54<1776:53:53, 64.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 73: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 74/100000 [51:01<1305:41:58, 47.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 74: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 75/100000 [51:09<976:43:36, 35.19s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 75: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 76/100000 [51:17<757:17:55, 27.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 76: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 77/100000 [51:30<634:23:06, 22.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 77: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 78/100000 [51:47<588:14:59, 21.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 78: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 79/100000 [51:54<464:47:03, 16.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 79: Summed_Reward = 12.0\n",
            "Episode 80: Summed_Reward = 29.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 81/100000 [1:01:51<3735:40:12, 134.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 81: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 82/100000 [1:02:11<2783:29:27, 100.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 82: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 83/100000 [1:02:22<2032:26:24, 73.23s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 83: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 84/100000 [1:02:40<1580:04:49, 56.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 84: Summed_Reward = 29.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 85/100000 [1:02:52<1200:41:00, 43.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 85: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 86/100000 [1:03:12<1005:51:31, 36.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 86: Summed_Reward = 37.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 87/100000 [1:03:21<780:57:13, 28.14s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 87: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 88/100000 [1:03:29<612:56:59, 22.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 88: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 89/100000 [1:03:37<500:24:54, 18.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 89: Summed_Reward = 16.0\n",
            "Episode 90: Summed_Reward = 35.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 91/100000 [1:15:11<4344:51:21, 156.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 91: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 92/100000 [1:15:22<3133:19:57, 112.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 92: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 93/100000 [1:15:35<2302:34:05, 82.97s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 93: Summed_Reward = 21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 94/100000 [1:15:45<1699:25:48, 61.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 94: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 95/100000 [1:15:56<1281:08:31, 46.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 95: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 96/100000 [1:16:13<1032:37:09, 37.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 96: Summed_Reward = 31.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 97/100000 [1:16:22<797:54:15, 28.75s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 97: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 98/100000 [1:16:28<611:17:04, 22.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 98: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 99/100000 [1:16:53<634:30:07, 22.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 99: Summed_Reward = 46.0\n",
            "Episode 100: Summed_Reward = 65.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 101/100000 [1:29:55<4890:47:36, 176.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 101: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 102/100000 [1:30:02<3482:17:35, 125.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 102: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 103/100000 [1:30:10<2499:39:12, 90.08s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 103: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 104/100000 [1:30:24<1868:11:01, 67.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 104: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 105/100000 [1:30:33<1382:39:49, 49.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 105: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 106/100000 [1:30:48<1095:17:25, 39.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 106: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 107/100000 [1:31:10<951:02:33, 34.27s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 107: Summed_Reward = 41.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 108/100000 [1:31:28<812:36:37, 29.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 108: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 109/100000 [1:31:37<645:12:55, 23.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 109: Summed_Reward = 17.0\n",
            "Episode 110: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 111/100000 [1:45:43<5274:49:31, 190.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 111: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 112/100000 [1:45:51<3766:21:07, 135.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 112: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 113/100000 [1:45:57<2682:48:51, 96.69s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 113: Summed_Reward = 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 114/100000 [1:46:04<1934:55:42, 69.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 114: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 115/100000 [1:46:23<1511:52:36, 54.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 115: Summed_Reward = 36.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 116/100000 [1:46:52<1304:17:17, 47.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 116: Summed_Reward = 55.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 117/100000 [1:47:24<1174:58:56, 42.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 117: Summed_Reward = 60.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 118/100000 [1:47:41<968:47:33, 34.92s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 118: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 119/100000 [1:47:53<773:52:37, 27.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 119: Summed_Reward = 21.0\n",
            "Episode 120: Summed_Reward = 42.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 121/100000 [2:03:49<5982:24:45, 215.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 121: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 122/100000 [2:04:09<4350:18:19, 156.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 122: Summed_Reward = 37.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 123/100000 [2:04:25<3178:42:41, 114.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 123: Summed_Reward = 31.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 124/100000 [2:04:33<2290:24:39, 82.56s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 124: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 125/100000 [2:04:44<1699:00:33, 61.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 125: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 126/100000 [2:04:52<1252:04:29, 45.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 126: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 127/100000 [2:05:02<959:44:39, 34.59s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 127: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 128/100000 [2:05:38<971:54:48, 35.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 128: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 129/100000 [2:05:58<852:30:13, 30.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 129: Summed_Reward = 36.0\n",
            "Episode 130: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 131/100000 [2:23:57<6778:28:47, 244.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 131: Summed_Reward = 52.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 132/100000 [2:24:15<4894:14:53, 176.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 132: Summed_Reward = 30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 133/100000 [2:24:40<3631:10:34, 130.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 133: Summed_Reward = 43.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 134/100000 [2:24:51<2633:34:30, 94.94s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 134: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 135/100000 [2:24:57<1896:05:29, 68.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 135: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 136/100000 [2:25:06<1403:49:55, 50.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 136: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 137/100000 [2:25:20<1094:39:56, 39.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 137: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 138/100000 [2:25:34<884:03:45, 31.87s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 138: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 139/100000 [2:26:02<854:43:16, 30.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 139: Summed_Reward = 50.0\n",
            "Episode 140: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 141/100000 [2:45:04<7112:21:13, 256.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 141: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 142/100000 [2:45:36<5239:37:47, 188.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 142: Summed_Reward = 48.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 143/100000 [2:45:47<3765:18:29, 135.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 143: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 144/100000 [2:46:01<2748:57:26, 99.11s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 144: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 145/100000 [2:46:10<1998:55:06, 72.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 145: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 146/100000 [2:46:15<1443:24:16, 52.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 146: Summed_Reward = 9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 147/100000 [2:46:53<1324:31:00, 47.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 147: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 148/100000 [2:47:21<1156:32:23, 41.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 148: Summed_Reward = 49.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 149/100000 [2:47:29<882:03:16, 31.80s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 149: Summed_Reward = 15.0\n",
            "Episode 150: Summed_Reward = 20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 151/100000 [3:08:14<7717:14:15, 278.24s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 151: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 152/100000 [3:08:28<5516:54:34, 198.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 152: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 153/100000 [3:08:34<3918:43:43, 141.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 153: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 154/100000 [3:09:30<3202:20:46, 115.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 154: Summed_Reward = 99.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 155/100000 [3:09:39<2321:40:14, 83.71s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 155: Summed_Reward = 17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 156/100000 [3:09:48<1698:22:10, 61.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 156: Summed_Reward = 15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 157/100000 [3:09:57<1263:15:49, 45.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 157: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 158/100000 [3:10:25<1117:09:30, 40.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 158: Summed_Reward = 49.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 159/100000 [3:11:02<1093:57:38, 39.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 159: Summed_Reward = 66.0\n",
            "Episode 160: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 161/100000 [3:34:00<8584:47:47, 309.55s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 161: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 162/100000 [3:34:45<6384:58:57, 230.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 162: Summed_Reward = 80.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 163/100000 [3:34:59<4585:36:13, 165.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 163: Summed_Reward = 25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 164/100000 [3:35:30<3467:39:10, 125.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 164: Summed_Reward = 55.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 165/100000 [3:35:56<2639:46:59, 95.19s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 165: Summed_Reward = 45.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 166/100000 [3:36:05<1924:17:51, 69.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 166: Summed_Reward = 16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 167/100000 [3:36:27<1530:27:00, 55.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 167: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 168/100000 [3:37:16<1479:59:17, 53.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 168: Summed_Reward = 86.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 169/100000 [3:37:31<1160:24:30, 41.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 169: Summed_Reward = 26.0\n",
            "Episode 170: Summed_Reward = 81.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 171/100000 [4:03:21<9612:44:23, 346.65s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 171: Summed_Reward = 12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 172/100000 [4:04:18<7199:03:33, 259.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 172: Summed_Reward = 91.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 173/100000 [4:04:37<5200:48:50, 187.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 173: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 174/100000 [4:05:08<3897:13:17, 140.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 174: Summed_Reward = 55.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 175/100000 [4:05:30<2909:39:20, 104.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 175: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 176/100000 [4:05:53<2227:56:40, 80.35s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 176: Summed_Reward = 41.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 177/100000 [4:06:19<1773:27:48, 63.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 177: Summed_Reward = 45.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 178/100000 [4:06:41<1426:47:23, 51.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 178: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 179/100000 [4:06:47<1048:07:56, 37.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 179: Summed_Reward = 11.0\n",
            "Episode 180: Summed_Reward = 57.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 181/100000 [4:35:22<10546:07:53, 380.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 181: Summed_Reward = 32.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 182/100000 [4:35:30<7445:16:16, 268.52s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 182: Summed_Reward = 13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 183/100000 [4:36:08<5533:38:53, 199.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 183: Summed_Reward = 68.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 184/100000 [4:36:27<4027:08:33, 145.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 184: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 185/100000 [4:37:05<3133:11:22, 113.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 185: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 186/100000 [4:37:13<2260:51:22, 81.54s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 186: Summed_Reward = 14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 187/100000 [4:37:23<1667:44:05, 60.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 187: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 188/100000 [4:37:46<1354:01:18, 48.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 188: Summed_Reward = 39.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 189/100000 [4:37:56<1038:14:11, 37.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 189: Summed_Reward = 19.0\n",
            "Episode 190: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 191/100000 [5:07:38<10905:43:41, 393.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 191: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 192/100000 [5:08:01<7828:35:20, 282.37s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 192: Summed_Reward = 40.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 193/100000 [5:08:31<5724:44:49, 206.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 193: Summed_Reward = 52.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 194/100000 [5:08:57<4225:04:07, 152.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 194: Summed_Reward = 47.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 195/100000 [5:09:26<3203:24:42, 115.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 195: Summed_Reward = 53.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 196/100000 [5:09:54<2472:43:43, 89.19s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 196: Summed_Reward = 48.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 197/100000 [5:10:31<2042:28:11, 73.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 197: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 198/100000 [5:11:09<1741:40:42, 62.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 198: Summed_Reward = 66.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 199/100000 [5:11:53<1585:14:29, 57.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 199: Summed_Reward = 77.0\n",
            "Episode 200: Summed_Reward = 38.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 201/100000 [5:44:14<12091:39:44, 436.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 201: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 202/100000 [5:44:29<8589:21:25, 309.84s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 202: Summed_Reward = 27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 203/100000 [5:45:30<6519:43:16, 235.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 203: Summed_Reward = 111.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 204/100000 [5:45:42<4666:14:06, 168.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 204: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 205/100000 [5:46:01<3423:47:01, 123.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 205: Summed_Reward = 34.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 206/100000 [5:46:16<2518:08:23, 90.84s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 206: Summed_Reward = 26.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 207/100000 [5:46:29<1873:03:57, 67.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 207: Summed_Reward = 24.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 208/100000 [5:46:35<1362:56:57, 49.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 208: Summed_Reward = 11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 209/100000 [5:47:07<1216:20:27, 43.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 209: Summed_Reward = 57.0\n",
            "Episode 210: Summed_Reward = 22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 211/100000 [6:22:32<13115:41:15, 473.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 211: Summed_Reward = 106.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 212/100000 [6:22:45<9287:41:56, 335.07s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 212: Summed_Reward = 23.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 213/100000 [6:23:04<6661:03:09, 240.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 213: Summed_Reward = 33.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 214/100000 [6:23:51<5051:48:50, 182.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 214: Summed_Reward = 84.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 215/100000 [6:24:28<3844:54:00, 138.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 215: Summed_Reward = 65.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 216/100000 [6:24:48<2854:15:04, 102.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 216: Summed_Reward = 34.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 217/100000 [6:25:12<2201:09:20, 79.41s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 217: Summed_Reward = 44.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 218/100000 [6:26:29<2180:26:06, 78.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 218: Summed_Reward = 136.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 219/100000 [6:27:11<1878:46:28, 67.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 219: Summed_Reward = 75.0\n",
            "Episode 220: Summed_Reward = 32.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 221/100000 [7:05:29<14320:00:44, 516.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 221: Summed_Reward = 18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 222/100000 [7:05:40<10111:50:47, 364.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 222: Summed_Reward = 19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 223/100000 [7:07:00<7745:31:10, 279.46s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 223: Summed_Reward = 144.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 224/100000 [7:07:25<5626:53:54, 203.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 224: Summed_Reward = 44.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 225/100000 [7:08:33<4506:08:59, 162.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 225: Summed_Reward = 118.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 226/100000 [7:09:06<3430:51:02, 123.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 226: Summed_Reward = 57.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 227/100000 [7:10:41<3190:29:41, 115.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 227: Summed_Reward = 168.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 228/100000 [7:11:07<2452:35:03, 88.49s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 228: Summed_Reward = 45.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 229/100000 [7:12:00<2155:57:15, 77.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 229: Summed_Reward = 82.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 229/100000 [7:13:29<3147:45:38, 113.58s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m pre_activated_actions \u001b[38;5;241m=\u001b[39m initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n\u001b[0;32m     27\u001b[0m pre_activated_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pre_activated_actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 28\u001b[0m pre_activated_actions \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_pre_activated_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_for_deducing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mmodel_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mpre_activated_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mdesired_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m action_argmax    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(pre_activated_actions[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     35\u001b[0m action           \u001b[38;5;241m=\u001b[39m vectorizing_action(action_size, action_argmax)\n",
            "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mupdate_pre_activated_actions\u001b[1;34m(epoch_for_deducing, model_loader, state, pre_activated_actions, desired_reward, beta)\u001b[0m\n\u001b[0;32m     23\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function\n\u001b[1;32m---> 26\u001b[0m output        \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m total_loss    \u001b[38;5;241m=\u001b[39m loss_function(output, desired_reward)\n\u001b[0;32m     28\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# get grad\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[4], line 101\u001b[0m, in \u001b[0;36mbuild_model.forward\u001b[1;34m(self, initial_hidden, x, padding_mask)\u001b[0m\n\u001b[0;32m     99\u001b[0m     out, h   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_layer_0(out, (h, h))\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     out, h   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_layer_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m out, _     \u001b[38;5;241m=\u001b[39m rnn_utils\u001b[38;5;241m.\u001b[39mpad_packed_sequence(out, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m padding    \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_sequence_size \u001b[38;5;241m-\u001b[39m out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\M\\AppData\\Local\\anaconda3\\envs\\Genrl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1142\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1139\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1140\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1142\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1145\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, 0])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env           = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state         = env.reset()\n",
        "    summed_reward = 0\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state)\n",
        "    short_term_state_list.append(state[0])\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # initializing and updating actions\n",
        "        state                 = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = update_pre_activated_actions(epoch_for_deducing,\n",
        "                                                             model_loader,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "        action           = vectorizing_action(action_size, action_argmax)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_argmax)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state)\n",
        "        short_term_state_list.append(state[0])\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode+1, summed_reward])\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentialize short term experience replay buffer and then save it to long term experience replay buffer\n",
        "    short_term_sequentialized_state_list, \\\n",
        "    short_term_sequentialized_actions_list, \\\n",
        "    short_term_sequentialized_reward_list, \\\n",
        "    short_term_sequentialized_next_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, time_size )\n",
        "    short_term_sequentialized_state_tensor,\\\n",
        "    short_term_sequentialized_actions_tensor,\\\n",
        "    short_term_sequentialized_reward_tensor,\\\n",
        "    short_term_sequentialized_next_state_tensor,\\\n",
        "    short_term_sequentialized_padding_mask = obtain_tensor_from_list(short_term_sequentialized_state_list,\n",
        "                                                                     short_term_sequentialized_actions_list,\n",
        "                                                                     short_term_sequentialized_reward_list,\n",
        "                                                                     short_term_sequentialized_next_state_list,\n",
        "                                                                     device,\n",
        "                                                                     time_size,\n",
        "                                                                     mask_value,\n",
        "                                                                     num_heads)\n",
        "    if training_episode==0:\n",
        "        long_term_sequentialized_state_tensor      = copy.deepcopy(short_term_sequentialized_state_tensor)\n",
        "        long_term_sequentialized_actions_tensor    = copy.deepcopy(short_term_sequentialized_actions_tensor)\n",
        "        long_term_sequentialized_reward_tensor     = copy.deepcopy(short_term_sequentialized_reward_tensor)\n",
        "        long_term_sequentialized_next_state_tensor = copy.deepcopy(short_term_sequentialized_next_state_tensor)\n",
        "        long_term_sequentialized_padding_mask      = copy.deepcopy(short_term_sequentialized_padding_mask)\n",
        "    else:\n",
        "        long_term_sequentialized_state_tensor      = torch.cat((long_term_sequentialized_state_tensor     , short_term_sequentialized_state_tensor     ), dim=0)\n",
        "        long_term_sequentialized_actions_tensor    = torch.cat((long_term_sequentialized_actions_tensor   , short_term_sequentialized_actions_tensor   ), dim=0)\n",
        "        long_term_sequentialized_reward_tensor     = torch.cat((long_term_sequentialized_reward_tensor    , short_term_sequentialized_reward_tensor    ), dim=0)\n",
        "        long_term_sequentialized_next_state_tensor = torch.cat((long_term_sequentialized_next_state_tensor, short_term_sequentialized_next_state_tensor), dim=0)\n",
        "        long_term_sequentialized_padding_mask      = torch.cat((long_term_sequentialized_padding_mask     , short_term_sequentialized_padding_mask     ), dim=0)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # creating samples for prioritized experience replay buffer:\n",
        "        dataset      = TensorDataset(long_term_sequentialized_state_tensor     ,\n",
        "                                     long_term_sequentialized_actions_tensor   ,\n",
        "                                     long_term_sequentialized_reward_tensor    ,\n",
        "                                     long_term_sequentialized_next_state_tensor,\n",
        "                                     long_term_sequentialized_padding_mask     )\n",
        "        data_loader_ = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "        total_temporal_difference_error = 0\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "                temporal_difference_error        = update_error(model, \n",
        "                                                                data_loader_,\n",
        "                                                                device)\n",
        "                total_temporal_difference_error += temporal_difference_error\n",
        "        torch.cuda.synchronize()\n",
        "        total_temporal_difference_error = total_temporal_difference_error.cpu().numpy()\n",
        "        total_temporal_difference_error = total_temporal_difference_error / np.sum(total_temporal_difference_error)\n",
        "        index_list                      = np.random.choice(range(len(dataset)), \n",
        "                                                           size=min(PER_sample_size, len(dataset)), \n",
        "                                                           p=total_temporal_difference_error, \n",
        "                                                           replace=False)\n",
        "        selected_sequentialized_state_tensor,\\\n",
        "        selected_sequentialized_actions_tensor,\\\n",
        "        selected_sequentialized_reward_tensor,\\\n",
        "        selected_sequentialized_next_state_tensor,\\\n",
        "        selected_sequentialized_padding_mask = obtain_tensor_according_to_TD_error(long_term_sequentialized_state_tensor     ,\n",
        "                                                                                   long_term_sequentialized_actions_tensor   ,\n",
        "                                                                                   long_term_sequentialized_reward_tensor    ,\n",
        "                                                                                   long_term_sequentialized_next_state_tensor,\n",
        "                                                                                   long_term_sequentialized_padding_mask     ,\n",
        "                                                                                   index_list)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # starting learning with elastic weight control\n",
        "        dataset      = TensorDataset(selected_sequentialized_state_tensor,\n",
        "                                     selected_sequentialized_actions_tensor,\n",
        "                                     selected_sequentialized_reward_tensor,\n",
        "                                     selected_sequentialized_next_state_tensor,\n",
        "                                     selected_sequentialized_padding_mask)\n",
        "        data_loader  = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
        "        data_loader_ = DataLoader(dataset, batch_size = len(dataset), shuffle=False)\n",
        "        gradient_matrix_loader = [''] * len(model_loader)\n",
        "        for i, model in enumerate(model_loader):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "                model, gradient_matrix    = update_model(epoch_for_learning,\n",
        "                                                         model,\n",
        "                                                         data_loader,\n",
        "                                                         data_loader_,\n",
        "                                                         prev_model_loader[i],\n",
        "                                                         prev_gradient_matrix_loader[i],\n",
        "                                                         EWC_lambda)\n",
        "                model_loader[i]           = model\n",
        "                gradient_matrix_loader[i] = gradient_matrix\n",
        "        torch.cuda.synchronize()\n",
        "        prev_model_loader           = copy.deepcopy(model_loader)\n",
        "        prev_gradient_matrix_loader = copy.deepcopy(gradient_matrix_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_loader)):\n",
        "            torch.save(model_loader[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing (testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_loader = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        drop_rate,\n",
        "                        alpha,\n",
        "                        mask_value)\n",
        "    model.to(device)\n",
        "    model_loader.append(model)\n",
        "\n",
        "for i in range(len(model_loader)):\n",
        "    model_loader[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = np.atleast_2d(np.ones(reward_size))\n",
        "desired_reward = torch.tensor(desired_reward, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                 = torch.tensor(state, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_actions = torch.tensor(pre_activated_actions, dtype=torch.float).to(device)\n",
        "        pre_activated_actions = update_pre_activated_actions(epoch_for_deducing,\n",
        "                                                             model_loader,\n",
        "                                                             state,\n",
        "                                                             pre_activated_actions,\n",
        "                                                             desired_reward,\n",
        "                                                             beta)\n",
        "        action_argmax    = int(torch.argmax(pre_activated_actions[0, 0]))\n",
        "\n",
        "        state, reward, done,  info = env.step(action_argmax)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
